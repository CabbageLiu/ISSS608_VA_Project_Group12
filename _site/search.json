[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "project_proposal/project_proposal.html",
    "href": "project_proposal/project_proposal.html",
    "title": "Project Proposal",
    "section": "",
    "text": "Hi testing"
  },
  {
    "objectID": "project_proposal/Proposal_DC.html",
    "href": "project_proposal/Proposal_DC.html",
    "title": "Proposal",
    "section": "",
    "text": "‚ÄúWhen you have eliminated the impossible, whatever remains, however improbable, must be the truth.‚Äù\n‚Äî Sherlock Holmes\nThey call it paradise. Oceanus ‚Äî a remote island once known for its fishing heritage ‚Äî now hosts yacht parties, drone shoots, and conservation protests. But for me, Clepper Jessen, it‚Äôs something else entirely: a mystery waiting to be unravelled.\nTwo weeks ago, something changed. Nemo Reef was abruptly shut down. Sailor Shift‚Äôs glamorous arrival was followed by a sudden storm of encrypted radio chatter, midnight movements, and fast-tracked approvals. Names I thought were long gone began to reappear ‚Äî only this time, hidden behind pseudonyms like ‚ÄúThe Boss‚Äù and ‚ÄúThe Lookout.‚Äù\nWith my intern, I intercepted and parsed every message I could get my hands on. But a pile of transcripts, no matter how detailed, is like a thousand-piece puzzle without the picture on the box. I needed a new tool ‚Äî one that could see what I couldn‚Äôt and connect what others wouldn‚Äôt.\nThat‚Äôs where this application comes in.\nThis is no ordinary dashboard. It is an investigative lens ‚Äî built to:\n\nUncover temporal anomalies that hint at coordination,\nTrace power and influence through hidden networks,\nDecode aliases and reveal who‚Äôs hiding behind what name,\nSurface clusters of collaboration, dissent, or manipulation, and\nPiece together a case around suspects like Nadia Conti, whose past criminal ties may not be so past after all.\n\nWith every filter I apply and every link I explore, I come closer to the truth.\nOceanus wants to forget. My team is here to remember.\n\n\n\nOceanus is at a crossroads. Once a tranquil fishing haven, it now stands as a microcosm of modern tension ‚Äî a battleground between commercial tourism, environmental activism, and quiet corruption. The sudden influx of celebrities, bureaucrats, and shadowy middlemen has triggered suspicion, particularly after the abrupt closure of Nemo Reef.\nAs an investigative journalist, I‚Äôve seen patterns before. But this one? It‚Äôs tangled in layers of aliases, logistics, and unlikely partnerships. That‚Äôs why I needed a visual analytics tool ‚Äî not just to read the data, but to interrogate it.\nThis project is driven by a desire to democratize investigative intelligence. Through interactive and intuitive visuals, it empowers those like me ‚Äî and anyone who seeks truth ‚Äî to:\n\nDetect daily patterns that ordinary eyes miss,\nFollow the thread of influence across vessels, people, and organizations,\nReveal pseudonyms and hold disguised actors accountable,\nMap out group behaviour and thematic alliances, and\nCollect visual evidence that separates fact from fabrication ‚Äî especially in the case of Nadia Conti.\n\nBecause data may tell a story. But visual analytics lets us solve the case.\n\n\n\nThe intercepted radio communications have been transformed into a knowledge graph, capturing two weeks of activity between individuals, vessels, and organizations in Oceanus. Each node represents an entity, while edges represent relationships such as communication, approvals, or co-mentioning events. This knowledge graph also captures metadata like timestamps, topics, and pseudonyms.\nClepper Jessen suspects that behind the benign front of ocean tourism lies an orchestrated effort involving corruption, covert coordination, and manipulation of identity. Our task is to convert this static graph into an interactive investigative tool that surfaces hidden patterns and supports journalistic inquiry.\nInvestigative Problems include:\nüî∏ Temporal Anomalies & Daily Rhythms\nMessages cluster around the same time daily, suggesting habitual or coordinated behaviour. Tracking shifts over time may expose operational changes before and after critical events (e.g., the filming announcement).\nGoal: Uncover periodic communication spikes, shifts in timing patterns, and correlation to entity activity.\nüî∏Influence Mapping & Relationship Flows\nPower dynamics can be inferred from communication directionality and frequency. Entities like Sailor Shift, officials, and conservationists may be central nodes with high influence.\nGoal: Use graph metrics (e.g., betweenness, eigenvector centrality) to identify influencers and track their evolving roles.\nüî∏Group Affiliation & Topic Clustering\nEntities form clusters ‚Äî such as the Green Guardians or celebrity entourages ‚Äî based on shared contacts or themes. Group-specific topics and interaction patterns can reveal operational roles.\nGoal: Detect communities using clustering algorithms and associate them with dominant discussion themes (e.g., tourism logistics vs.¬†environmental protests).\nüî∏Pseudonym Usage & Identity Masking\nPseudonyms like ‚ÄúThe Boss‚Äù or ‚ÄúThe Lookout‚Äù conceal true identities and complicate accountability. Some aliases may be reused across actors or tied to specific event types.\nGoal: Identify single or multi-user pseudonyms, map their activity and cross-compare behaviourally with known entities to uncover deception.\nüî∏Evidence of Misconduct ‚Äì Nadia Conti Focus\nClepper believes Nadia Conti may be covertly operating under new identities or aliases. Her history with illegal fishing makes her a person of interest in new corruption schemes.\nGoal: Map Nadia‚Äôs communication footprint, link indirect relationships via pseudonyms, and surface anomalies suggesting concealment or collusion.\n\n\n\nWe will develop a modular Shiny web app to convert the static knowledge graph into an interactive, investigative interface tailored to Clepper‚Äôs needs. Each module corresponds to a key problem statement.\nModule A: Temporal Pattern Explorer\nGoal: Visualize daily cycles and anomalies in communication frequency and topic intensity.\n\nInput: Message timestamp, sender, topic\nVisualization:\n\n-   Heatmap (Hour of day √ó Day) showing volume of messages\n-   Line chart overlay per sender/group (faceted if needed)\n-   Interactive brush to zoom into specific dates/times\n\nAdditional Feature: Compare activity of individual entities vs.¬†global baseline\n\nModule B: Entity Influence Dashboard\nGoal: Reveal who influences whom across people, vessels, and organizations.\n\nInput: Entity-entity edges, directionality, frequency, topic weight\nBackend:\n\nCompute centrality metrics (degree, betweenness, eigenvector)\nFilter by direction, topic, or message count\n\nVisualization:\n\nInteractive visNetwork graph with dynamic node sizing (influence) and colouring (group affiliation or topic)\nClickable entity profile cards that update to show:\n\nTop connections\nCommunication timeline\nTopics discussed\n\n\n\nModule C: Pseudonym Tracker\nGoal: Detect and decode pseudonym usage and role masking.\n\nInput: Entity-pseudonym mappings, message co-occurrence, behavioural similarity\nAlgorithm:\n\nIdentify pseudonyms via string detection, co-use clustering\nInfer multi-usage by examining shared communication patterns\n\nVisualization:\n\nNode-link diagram with toggle between ‚Äúreal‚Äù and ‚Äúalias‚Äù modes\nHighlight shared aliases and suspicious handoffs of pseudonym identity\nTextual summary of evidence linking alias to probable identity\n\n\nModule D: Community & Topic Association\nGoal: Uncover hidden group structures and topic themes.\n\nBackend:\n\nApply Louvain clustering to full entity graph\nTag clusters with dominant communication themes (NLP topic tagging or keyword extraction)\n\nVisualization:\n\nFaceted layout by cluster: each showing node graph + summary\nBar chart of top topics per group\nHover tooltips to explain inter-group bridges (e.g., actors linking Sailor Shift‚Äôs crew with Green Guardians)\n\nModule E: Nadia Conti Activity Tracker\n\nGoal: Gather and visualize evidence of Nadia‚Äôs potential wrongdoing.\n\nInput: All messages involving or referring to Nadia or her aliases\nBackend:\n\nCreate Nadia subgraph: direct links, pseudonym usage, temporal footprint\nCompute abnormal communication patterns vs.¬†baseline\n\nVisualization:\n\nTimeline of Nadia‚Äôs communications: who, when, and topic\nHighlight high-risk pseudonym periods or clandestine approvals\nSummary insight box: ‚ÄúEvidence Level: Moderate/Strong/None‚Äù\n\n\n\n\n\n\nFully interactive network graph with edge filters (time, entity, topic)\nDynamic search for people, vessels, or pseudonyms\nSide panels showing metadata and inferred roles\nDownloadable visual summaries\nStoryboarding feature: Clepper can save key frames or views for investigation reports\n\n\n\n\nüîπ 6.1 Home (Overview & Narrative Context)\n\nPurpose: Anchor the user with contextual background and investigative framing.\nComponents:\n\nShort narrative summary with quote from Clepper\nOptional interactive map of Oceanus (for future enhancement)\nInfoboxes showing key facts (e.g., number of entities, time span, known pseudonyms)\nNavigation buttons or sidebar leading to investigative tabs\n\nValue: Orients the user to the setting and stakes of the case; sets an investigative tone.\n\nüîπ 6.2 Timeline Explorer Tab\n\nPurpose: Uncover daily and hourly communication patterns across the two-week observation window.\nComponents:\n\nüìÖ Heatmap: Date √ó Hour grid showing communication intensity\nüìà Line charts: Message frequency over time (overall or by entity/group)\nüîé Interactive filters: Select entity type (people, vessels), specific names, or keyword topics\nüß≠ Timeline brushing: Zoom in on specific dates to transition into more detailed tab\n\nValue: Highlights coordination patterns and anomalies\n\nüîπ 6.3 Influence Drilldown Tab\n\nPurpose: Map out who holds power, and how they influence others within the knowledge graph.\nComponents:\n\nüï∏Ô∏è Interactive visNetwork graph:\n\nNodes sized by centrality (influence)\nDirected edges colored by topic category (e.g., environmental, celebrity, logistics)\n\nüîÅ Switch view: ‚ÄúInbound‚Äù vs.¬†‚ÄúOutbound‚Äù influence\nüìä Side panel: Top 5 influencers, ego network for selected entity\nüóÇÔ∏è Entity profile card: When clicked, show timeline of messages, role type, key connection\n\nValue: Supports influence tracing and actor prioritization, especially for figures like Sailor Shift, conservation leaders, and anonymous bureaucrats.\n\nüîπ 6.4 Pseudonym Tracker Tab\n\nPurpose: Detect and decode the use of aliases and multi-identity patterns.\nComponents:\n\nüë§ Alias network: Links pseudonyms to likely real-world identities based on co-activity\nüß¨ Shared pseudonym highlighting (multiple entities using ‚ÄúBoss‚Äù at different hours)\nüìÖ Pseudonym timeline: Track when and how often each alias was used\nüõ†Ô∏è Optional toggle: Show/hide pseudonyms in other tabs (global control)\n\nValue: Aids in exposing hidden actors and strengthens the foundation for behavioral de-anonymization.\n\nüîπ 6.5 Community & Topic Cluster Tab\n\nPurpose: Visualize thematic groupings and entity communities within the network.\nComponents:\n\nüß† Clustered graph layout (Louvain or Walktrap output)\nüé® Color-coded communities with legend (e.g., Green = Environmentalists, Blue = Leisure Vessels)\nüìö Wordcloud or bar chart: Top keywords or message themes per group\nüîó Bridge node detection: Show entities linking multiple clusters\n\nValue: Supports exploratory investigation into potential collaboration or conflict between stakeholder groups.\n\nüîπ 6.6 Nadia Conti Case Dashboard\n\nPurpose: Build a narrative case file centered around Nadia Conti.\nComponents:\n\nüîç Nadia‚Äôs entity profile + aliases\nüìä Summary panel: Activity volume, indirect connections, known pseudonyms\n‚è≥ Timeline view: Messages involving or referencing Nadia\nüß© Inference section: Summary of potential illicit actions or suspicious interactions\n\nValue: Synthesizes cross-tab insights to evaluate the core investigative hypothesis: Is Nadia still engaging in illegal activity?\n\nüîπ 6.7 (Optional) Evidence Builder Tab\n\nPurpose: Enable Clepper or the user to export selected visuals and insights for report writing or news publishing.\nComponents:\n\nüìå Bookmark key views from any tab\nüìù Generate narrative summaries or ‚Äúcase snapshots‚Äù\nüì§ Export PDF/PNG visuals\n\nValue: Supports storytelling, publication, or handover to law enforcement or editorial teams.\n\n\n\n\nThis project proposes a purpose-built, investigative visual analytics application designed to illuminate the hidden relationships, pseudonym usage, and potential corruption embedded in the Oceanus radio communication dataset. By combining interactive graph-based exploration with temporal and entity-level analysis, the tool enables investigative users like Clepper Jessen to transition from static information to dynamic, evidence-based storytelling.\nBeyond solving the specific case of Nadia Conti and the suspicious activity around Nemo Reef, this application demonstrates how visual analytics can serve as a powerful ally in data-driven journalism, enabling the detection of influence, identity masking, and organized behaviour in semi-structured communications. Its modular design ensures adaptability for future investigative contexts, whether in civic watchdog efforts, environmental intelligence, or geopolitical monitoring.\nUltimately, this project aligns with the broader goal of democratizing data and analytics, offering a transparent and intuitive interface for domain experts to extract actionable insights from complex knowledge graphs ‚Äî and to do so without requiring technical expertise in coding or graph theory."
  },
  {
    "objectID": "project_proposal/Proposal_DC.html#introduction",
    "href": "project_proposal/Proposal_DC.html#introduction",
    "title": "Proposal",
    "section": "",
    "text": "‚ÄúWhen you have eliminated the impossible, whatever remains, however improbable, must be the truth.‚Äù\n‚Äî Sherlock Holmes\nThey call it paradise. Oceanus ‚Äî a remote island once known for its fishing heritage ‚Äî now hosts yacht parties, drone shoots, and conservation protests. But for me, Clepper Jessen, it‚Äôs something else entirely: a mystery waiting to be unravelled.\nTwo weeks ago, something changed. Nemo Reef was abruptly shut down. Sailor Shift‚Äôs glamorous arrival was followed by a sudden storm of encrypted radio chatter, midnight movements, and fast-tracked approvals. Names I thought were long gone began to reappear ‚Äî only this time, hidden behind pseudonyms like ‚ÄúThe Boss‚Äù and ‚ÄúThe Lookout.‚Äù\nWith my intern, I intercepted and parsed every message I could get my hands on. But a pile of transcripts, no matter how detailed, is like a thousand-piece puzzle without the picture on the box. I needed a new tool ‚Äî one that could see what I couldn‚Äôt and connect what others wouldn‚Äôt.\nThat‚Äôs where this application comes in.\nThis is no ordinary dashboard. It is an investigative lens ‚Äî built to:\n\nUncover temporal anomalies that hint at coordination,\nTrace power and influence through hidden networks,\nDecode aliases and reveal who‚Äôs hiding behind what name,\nSurface clusters of collaboration, dissent, or manipulation, and\nPiece together a case around suspects like Nadia Conti, whose past criminal ties may not be so past after all.\n\nWith every filter I apply and every link I explore, I come closer to the truth.\nOceanus wants to forget. My team is here to remember."
  },
  {
    "objectID": "project_proposal/Proposal_DC.html#motivation",
    "href": "project_proposal/Proposal_DC.html#motivation",
    "title": "Proposal",
    "section": "",
    "text": "Oceanus is at a crossroads. Once a tranquil fishing haven, it now stands as a microcosm of modern tension ‚Äî a battleground between commercial tourism, environmental activism, and quiet corruption. The sudden influx of celebrities, bureaucrats, and shadowy middlemen has triggered suspicion, particularly after the abrupt closure of Nemo Reef.\nAs an investigative journalist, I‚Äôve seen patterns before. But this one? It‚Äôs tangled in layers of aliases, logistics, and unlikely partnerships. That‚Äôs why I needed a visual analytics tool ‚Äî not just to read the data, but to interrogate it.\nThis project is driven by a desire to democratize investigative intelligence. Through interactive and intuitive visuals, it empowers those like me ‚Äî and anyone who seeks truth ‚Äî to:\n\nDetect daily patterns that ordinary eyes miss,\nFollow the thread of influence across vessels, people, and organizations,\nReveal pseudonyms and hold disguised actors accountable,\nMap out group behaviour and thematic alliances, and\nCollect visual evidence that separates fact from fabrication ‚Äî especially in the case of Nadia Conti.\n\nBecause data may tell a story. But visual analytics lets us solve the case."
  },
  {
    "objectID": "project_proposal/Proposal_DC.html#problem-statement",
    "href": "project_proposal/Proposal_DC.html#problem-statement",
    "title": "Proposal",
    "section": "",
    "text": "The intercepted radio communications have been transformed into a knowledge graph, capturing two weeks of activity between individuals, vessels, and organizations in Oceanus. Each node represents an entity, while edges represent relationships such as communication, approvals, or co-mentioning events. This knowledge graph also captures metadata like timestamps, topics, and pseudonyms.\nClepper Jessen suspects that behind the benign front of ocean tourism lies an orchestrated effort involving corruption, covert coordination, and manipulation of identity. Our task is to convert this static graph into an interactive investigative tool that surfaces hidden patterns and supports journalistic inquiry.\nInvestigative Problems include:\nüî∏ Temporal Anomalies & Daily Rhythms\nMessages cluster around the same time daily, suggesting habitual or coordinated behaviour. Tracking shifts over time may expose operational changes before and after critical events (e.g., the filming announcement).\nGoal: Uncover periodic communication spikes, shifts in timing patterns, and correlation to entity activity.\nüî∏Influence Mapping & Relationship Flows\nPower dynamics can be inferred from communication directionality and frequency. Entities like Sailor Shift, officials, and conservationists may be central nodes with high influence.\nGoal: Use graph metrics (e.g., betweenness, eigenvector centrality) to identify influencers and track their evolving roles.\nüî∏Group Affiliation & Topic Clustering\nEntities form clusters ‚Äî such as the Green Guardians or celebrity entourages ‚Äî based on shared contacts or themes. Group-specific topics and interaction patterns can reveal operational roles.\nGoal: Detect communities using clustering algorithms and associate them with dominant discussion themes (e.g., tourism logistics vs.¬†environmental protests).\nüî∏Pseudonym Usage & Identity Masking\nPseudonyms like ‚ÄúThe Boss‚Äù or ‚ÄúThe Lookout‚Äù conceal true identities and complicate accountability. Some aliases may be reused across actors or tied to specific event types.\nGoal: Identify single or multi-user pseudonyms, map their activity and cross-compare behaviourally with known entities to uncover deception.\nüî∏Evidence of Misconduct ‚Äì Nadia Conti Focus\nClepper believes Nadia Conti may be covertly operating under new identities or aliases. Her history with illegal fishing makes her a person of interest in new corruption schemes.\nGoal: Map Nadia‚Äôs communication footprint, link indirect relationships via pseudonyms, and surface anomalies suggesting concealment or collusion."
  },
  {
    "objectID": "project_proposal/Proposal_DC.html#proposed-approach",
    "href": "project_proposal/Proposal_DC.html#proposed-approach",
    "title": "Proposal",
    "section": "",
    "text": "We will develop a modular Shiny web app to convert the static knowledge graph into an interactive, investigative interface tailored to Clepper‚Äôs needs. Each module corresponds to a key problem statement.\nModule A: Temporal Pattern Explorer\nGoal: Visualize daily cycles and anomalies in communication frequency and topic intensity.\n\nInput: Message timestamp, sender, topic\nVisualization:\n\n-   Heatmap (Hour of day √ó Day) showing volume of messages\n-   Line chart overlay per sender/group (faceted if needed)\n-   Interactive brush to zoom into specific dates/times\n\nAdditional Feature: Compare activity of individual entities vs.¬†global baseline\n\nModule B: Entity Influence Dashboard\nGoal: Reveal who influences whom across people, vessels, and organizations.\n\nInput: Entity-entity edges, directionality, frequency, topic weight\nBackend:\n\nCompute centrality metrics (degree, betweenness, eigenvector)\nFilter by direction, topic, or message count\n\nVisualization:\n\nInteractive visNetwork graph with dynamic node sizing (influence) and colouring (group affiliation or topic)\nClickable entity profile cards that update to show:\n\nTop connections\nCommunication timeline\nTopics discussed\n\n\n\nModule C: Pseudonym Tracker\nGoal: Detect and decode pseudonym usage and role masking.\n\nInput: Entity-pseudonym mappings, message co-occurrence, behavioural similarity\nAlgorithm:\n\nIdentify pseudonyms via string detection, co-use clustering\nInfer multi-usage by examining shared communication patterns\n\nVisualization:\n\nNode-link diagram with toggle between ‚Äúreal‚Äù and ‚Äúalias‚Äù modes\nHighlight shared aliases and suspicious handoffs of pseudonym identity\nTextual summary of evidence linking alias to probable identity\n\n\nModule D: Community & Topic Association\nGoal: Uncover hidden group structures and topic themes.\n\nBackend:\n\nApply Louvain clustering to full entity graph\nTag clusters with dominant communication themes (NLP topic tagging or keyword extraction)\n\nVisualization:\n\nFaceted layout by cluster: each showing node graph + summary\nBar chart of top topics per group\nHover tooltips to explain inter-group bridges (e.g., actors linking Sailor Shift‚Äôs crew with Green Guardians)\n\nModule E: Nadia Conti Activity Tracker\n\nGoal: Gather and visualize evidence of Nadia‚Äôs potential wrongdoing.\n\nInput: All messages involving or referring to Nadia or her aliases\nBackend:\n\nCreate Nadia subgraph: direct links, pseudonym usage, temporal footprint\nCompute abnormal communication patterns vs.¬†baseline\n\nVisualization:\n\nTimeline of Nadia‚Äôs communications: who, when, and topic\nHighlight high-risk pseudonym periods or clandestine approvals\nSummary insight box: ‚ÄúEvidence Level: Moderate/Strong/None‚Äù"
  },
  {
    "objectID": "project_proposal/Proposal_DC.html#expected-features",
    "href": "project_proposal/Proposal_DC.html#expected-features",
    "title": "Proposal",
    "section": "",
    "text": "Fully interactive network graph with edge filters (time, entity, topic)\nDynamic search for people, vessels, or pseudonyms\nSide panels showing metadata and inferred roles\nDownloadable visual summaries\nStoryboarding feature: Clepper can save key frames or views for investigation reports"
  },
  {
    "objectID": "project_proposal/Proposal_DC.html#early-wireframe-sketches-and-user-flow",
    "href": "project_proposal/Proposal_DC.html#early-wireframe-sketches-and-user-flow",
    "title": "Proposal",
    "section": "",
    "text": "üîπ 6.1 Home (Overview & Narrative Context)\n\nPurpose: Anchor the user with contextual background and investigative framing.\nComponents:\n\nShort narrative summary with quote from Clepper\nOptional interactive map of Oceanus (for future enhancement)\nInfoboxes showing key facts (e.g., number of entities, time span, known pseudonyms)\nNavigation buttons or sidebar leading to investigative tabs\n\nValue: Orients the user to the setting and stakes of the case; sets an investigative tone.\n\nüîπ 6.2 Timeline Explorer Tab\n\nPurpose: Uncover daily and hourly communication patterns across the two-week observation window.\nComponents:\n\nüìÖ Heatmap: Date √ó Hour grid showing communication intensity\nüìà Line charts: Message frequency over time (overall or by entity/group)\nüîé Interactive filters: Select entity type (people, vessels), specific names, or keyword topics\nüß≠ Timeline brushing: Zoom in on specific dates to transition into more detailed tab\n\nValue: Highlights coordination patterns and anomalies\n\nüîπ 6.3 Influence Drilldown Tab\n\nPurpose: Map out who holds power, and how they influence others within the knowledge graph.\nComponents:\n\nüï∏Ô∏è Interactive visNetwork graph:\n\nNodes sized by centrality (influence)\nDirected edges colored by topic category (e.g., environmental, celebrity, logistics)\n\nüîÅ Switch view: ‚ÄúInbound‚Äù vs.¬†‚ÄúOutbound‚Äù influence\nüìä Side panel: Top 5 influencers, ego network for selected entity\nüóÇÔ∏è Entity profile card: When clicked, show timeline of messages, role type, key connection\n\nValue: Supports influence tracing and actor prioritization, especially for figures like Sailor Shift, conservation leaders, and anonymous bureaucrats.\n\nüîπ 6.4 Pseudonym Tracker Tab\n\nPurpose: Detect and decode the use of aliases and multi-identity patterns.\nComponents:\n\nüë§ Alias network: Links pseudonyms to likely real-world identities based on co-activity\nüß¨ Shared pseudonym highlighting (multiple entities using ‚ÄúBoss‚Äù at different hours)\nüìÖ Pseudonym timeline: Track when and how often each alias was used\nüõ†Ô∏è Optional toggle: Show/hide pseudonyms in other tabs (global control)\n\nValue: Aids in exposing hidden actors and strengthens the foundation for behavioral de-anonymization.\n\nüîπ 6.5 Community & Topic Cluster Tab\n\nPurpose: Visualize thematic groupings and entity communities within the network.\nComponents:\n\nüß† Clustered graph layout (Louvain or Walktrap output)\nüé® Color-coded communities with legend (e.g., Green = Environmentalists, Blue = Leisure Vessels)\nüìö Wordcloud or bar chart: Top keywords or message themes per group\nüîó Bridge node detection: Show entities linking multiple clusters\n\nValue: Supports exploratory investigation into potential collaboration or conflict between stakeholder groups.\n\nüîπ 6.6 Nadia Conti Case Dashboard\n\nPurpose: Build a narrative case file centered around Nadia Conti.\nComponents:\n\nüîç Nadia‚Äôs entity profile + aliases\nüìä Summary panel: Activity volume, indirect connections, known pseudonyms\n‚è≥ Timeline view: Messages involving or referencing Nadia\nüß© Inference section: Summary of potential illicit actions or suspicious interactions\n\nValue: Synthesizes cross-tab insights to evaluate the core investigative hypothesis: Is Nadia still engaging in illegal activity?\n\nüîπ 6.7 (Optional) Evidence Builder Tab\n\nPurpose: Enable Clepper or the user to export selected visuals and insights for report writing or news publishing.\nComponents:\n\nüìå Bookmark key views from any tab\nüìù Generate narrative summaries or ‚Äúcase snapshots‚Äù\nüì§ Export PDF/PNG visuals\n\nValue: Supports storytelling, publication, or handover to law enforcement or editorial teams."
  },
  {
    "objectID": "project_proposal/Proposal_DC.html#conclusion",
    "href": "project_proposal/Proposal_DC.html#conclusion",
    "title": "Proposal",
    "section": "",
    "text": "This project proposes a purpose-built, investigative visual analytics application designed to illuminate the hidden relationships, pseudonym usage, and potential corruption embedded in the Oceanus radio communication dataset. By combining interactive graph-based exploration with temporal and entity-level analysis, the tool enables investigative users like Clepper Jessen to transition from static information to dynamic, evidence-based storytelling.\nBeyond solving the specific case of Nadia Conti and the suspicious activity around Nemo Reef, this application demonstrates how visual analytics can serve as a powerful ally in data-driven journalism, enabling the detection of influence, identity masking, and organized behaviour in semi-structured communications. Its modular design ensures adaptability for future investigative contexts, whether in civic watchdog efforts, environmental intelligence, or geopolitical monitoring.\nUltimately, this project aligns with the broader goal of democratizing data and analytics, offering a transparent and intuitive interface for domain experts to extract actionable insights from complex knowledge graphs ‚Äî and to do so without requiring technical expertise in coding or graph theory."
  },
  {
    "objectID": "TH3/Take-Home_Ex02_MC3.html",
    "href": "TH3/Take-Home_Ex02_MC3.html",
    "title": "Take-Home Assignment 2",
    "section": "",
    "text": "For the purpose of this assignment, five R packages will be used. They are tidyverse, jsonlite, tidygraph, ggraph and SmartEDA.\nIn the code chunk below, p_load() of pacman package is used to load the R packages into R environment.\n\npacman::p_load(tidyverse, jsonlite, \n               tidygraph, ggraph, SmartEDA, \n               ggrepel, scales, lubridate, dplyr, viridis)\n\n\n\n\nFor the purpose of this exercise, mc3.json file will be used. Before getting started, you should have the data set in the data sub-folder.\nIn the code chunk below, fromJSON() of jsonlite package is used to import mc3.json file into R and save the output object\n\nMC3 &lt;- fromJSON(\"data/MC3_graph.json\")\nMC3_schema &lt;- fromJSON(\"data/MC3_schema.json\")\n\n\n\n\nThe dataset was provided by VAST Challenge for MC3. This report utilizes two core datasets: MC3_graph.json, which encodes the knowledge graph of communications, events, and relationships; and MC3_schema.json, which defines the structure, subtypes, and attributes of each node and edge type within the graph. There ngraph contains a total of 1159 nodes and 3226 edges. Full description of node attributes and edge attributes is shown below.\nNodes Attributes are as such:\n\n\n\nNode Subtypes\n\n\nEdge Attributes are as such:\n\n\n\nNode-Edge-Node Matrix\n\n\n\n\n\nBefore preparing the data, it is always a good practice to examine the structure of mc3 knowledge graph.\nIn the code chunk below glimpse() is used to reveal the structure of mc3 knowledge graph.\n\nglimpse(MC3)\n\nList of 5\n $ directed  : logi TRUE\n $ multigraph: logi FALSE\n $ graph     :List of 4\n  ..$ mode        : chr \"static\"\n  ..$ edge_default: Named list()\n  ..$ node_default: Named list()\n  ..$ name        : chr \"VAST_MC3_Knowledge_Graph\"\n $ nodes     :'data.frame': 1159 obs. of  31 variables:\n  ..$ type             : chr [1:1159] \"Entity\" \"Entity\" \"Entity\" \"Entity\" ...\n  ..$ label            : chr [1:1159] \"Sam\" \"Kelly\" \"Nadia Conti\" \"Elise\" ...\n  ..$ name             : chr [1:1159] \"Sam\" \"Kelly\" \"Nadia Conti\" \"Elise\" ...\n  ..$ sub_type         : chr [1:1159] \"Person\" \"Person\" \"Person\" \"Person\" ...\n  ..$ id               : chr [1:1159] \"Sam\" \"Kelly\" \"Nadia Conti\" \"Elise\" ...\n  ..$ timestamp        : chr [1:1159] NA NA NA NA ...\n  ..$ monitoring_type  : chr [1:1159] NA NA NA NA ...\n  ..$ findings         : chr [1:1159] NA NA NA NA ...\n  ..$ content          : chr [1:1159] NA NA NA NA ...\n  ..$ assessment_type  : chr [1:1159] NA NA NA NA ...\n  ..$ results          : chr [1:1159] NA NA NA NA ...\n  ..$ movement_type    : chr [1:1159] NA NA NA NA ...\n  ..$ destination      : chr [1:1159] NA NA NA NA ...\n  ..$ enforcement_type : chr [1:1159] NA NA NA NA ...\n  ..$ outcome          : chr [1:1159] NA NA NA NA ...\n  ..$ activity_type    : chr [1:1159] NA NA NA NA ...\n  ..$ participants     : int [1:1159] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ thing_collected  :'data.frame':   1159 obs. of  2 variables:\n  .. ..$ type: chr [1:1159] NA NA NA NA ...\n  .. ..$ name: chr [1:1159] NA NA NA NA ...\n  ..$ reference        : chr [1:1159] NA NA NA NA ...\n  ..$ date             : chr [1:1159] NA NA NA NA ...\n  ..$ time             : chr [1:1159] NA NA NA NA ...\n  ..$ friendship_type  : chr [1:1159] NA NA NA NA ...\n  ..$ permission_type  : chr [1:1159] NA NA NA NA ...\n  ..$ start_date       : chr [1:1159] NA NA NA NA ...\n  ..$ end_date         : chr [1:1159] NA NA NA NA ...\n  ..$ report_type      : chr [1:1159] NA NA NA NA ...\n  ..$ submission_date  : chr [1:1159] NA NA NA NA ...\n  ..$ jurisdiction_type: chr [1:1159] NA NA NA NA ...\n  ..$ authority_level  : chr [1:1159] NA NA NA NA ...\n  ..$ coordination_type: chr [1:1159] NA NA NA NA ...\n  ..$ operational_role : chr [1:1159] NA NA NA NA ...\n $ edges     :'data.frame': 3226 obs. of  5 variables:\n  ..$ id         : chr [1:3226] \"2\" \"3\" \"5\" \"3013\" ...\n  ..$ is_inferred: logi [1:3226] TRUE FALSE TRUE TRUE TRUE TRUE ...\n  ..$ source     : chr [1:3226] \"Sam\" \"Sam\" \"Sam\" \"Sam\" ...\n  ..$ target     : chr [1:3226] \"Relationship_Suspicious_217\" \"Event_Communication_370\" \"Event_Assessment_600\" \"Relationship_Colleagues_430\" ...\n  ..$ type       : chr [1:3226] NA \"sent\" NA NA ...\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNotice that¬†Industry¬†field is in list data type. In general, this data type is not acceptable by¬†tbl_graph()¬†of¬†tidygraph. In order to avoid error arise when building tidygraph object, it is wiser to exclude this field from the edges data table. However, it might be still useful in subsequent analysis.\n\n\n\n\n\nNext,¬†as_tibble()¬†of¬†tibble¬†package package is used to extract the nodes and links tibble data frames from¬†mc3¬†tibble dataframe into two separate tibble dataframes called¬†mc3_nodes¬†and¬†mc3_edges¬†respectively.\n\nmc3_nodes &lt;- as_tibble(MC3$nodes)\nmc3_edges &lt;- as_tibble(MC3$edges)\n\n\n\n\nIt is time for us to apply appropriate EDA methods to examine the data.\nNodes:\n\nThe Code ChunkThe Plots\n\n\n\nExpCatViz(data=mc3_nodes,\n          col=\"lightblue\")\n\n\n\n\nExpCatViz(data=mc3_nodes,\n          col=\"lightblue\")\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\n\n\n\n[[7]]\n\n\n\n\n\n\n\n\n\n\n[[8]]\n\n\n\n\n\n\n\n\n\n\n[[9]]\n\n\n\n\n\n\n\n\n\n\n[[10]]\n\n\n\n\n\n\n\n\n\n\n[[11]]\n\n\n\n\n\n\n\n\n\n\n[[12]]\n\n\n\n\n\n\n\n\n\n\n[[13]]\n\n\n\n\n\n\n\n\n\n\n[[14]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat useful discovery can you obtained from the visualisation above?\n\n\n\n\n\nEdges:\n\nThe Code ChunkThe Plots\n\n\n\nExpCatViz(data=mc3_edges,\n          col=\"lightblue\")\n\n\n\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat useful discovery can you obtained from the visualisation above?\n\n\n\n\n\n\n\n\n\n\n\nCode chunk below performs the following data cleaning tasks:\n\nconvert values in id field into character data type,\nexclude records with id value are na,\nexclude records with similar id values,\nexclude thing_collected field, and\nsave the cleaned tibble dataframe into a new tibble datatable called mc3_nodes_cleaned.\n\n\n\nCode\nmc3_nodes_cleaned &lt;- mc3_nodes %&gt;%\n  mutate(id = as.character(id)) %&gt;%\n  filter(!is.na(id)) %&gt;%\n  distinct(id, .keep_all = TRUE) %&gt;%\n  select(-thing_collected)\n\n\n\n\n\nNext, the code chunk below will be used to:\n\nrename source and target fields to from_id and to_id respectively,\nconvert values in from_id and to_id fields to character data type,\nexclude values in from_id and to_id which not found in the id field of mc3_nodes_cleaned,\nexclude records whereby from_id and/or to_id values are missing, and\nsave the cleaned tibble dataframe and called it mc3_edges_cleaned.\n\n\n\nCode\nmc3_edges_cleaned &lt;- mc3_edges %&gt;%\n  rename(from_id = source, \n         to_id = target) %&gt;%\n  mutate(across(c(from_id, to_id), \n                as.character)) %&gt;%\n  filter(from_id %in% mc3_nodes_cleaned$id, \n         to_id %in% mc3_nodes_cleaned$id) %&gt;%\n  filter(!is.na(from_id), !is.na(to_id))\n\n\nNext, code chunk below will be used to create mapping of character id in¬†mc3_nodes_cleaned¬†to row index.\n\n\nCode\nnode_index_lookup &lt;- mc3_nodes_cleaned %&gt;%\n  mutate(.row_id = row_number()) %&gt;%\n  select(id, .row_id)\n\n\nNext, the code chunk below will be used to join and convert¬†from_id¬†and¬†to_id¬†to integer indices. At the same time we also drop rows with unmatched nodes.\n\n\nCode\nmc3_edges_indexed &lt;- mc3_edges_cleaned %&gt;%\n  left_join(node_index_lookup, \n            by = c(\"from_id\" = \"id\")) %&gt;%\n  rename(from = .row_id) %&gt;%\n  left_join(node_index_lookup, \n            by = c(\"to_id\" = \"id\")) %&gt;%\n  rename(to = .row_id) %&gt;%\n  select(from, to, is_inferred, type) %&gt;%\n  filter(!is.na(from) & !is.na(to))  \n\n\nNext the code chunk below is used to subset nodes to only those referenced by edges.\n\n\nCode\nused_node_indices &lt;- sort(\n  unique(c(mc3_edges_indexed$from, \n           mc3_edges_indexed$to)))\n\nmc3_nodes_final &lt;- mc3_nodes_cleaned %&gt;%\n  slice(used_node_indices) %&gt;%\n  mutate(new_index = row_number())\n\n\nWe will then use the code chunk below to rebuild lookup from old index to new index.\n\n\nCode\nold_to_new_index &lt;- tibble(\n  old_index = used_node_indices,\n  new_index = seq_along(\n    used_node_indices))\n\n\nLastly, the code chunk below will be used to update edge indices to match new node table.\n\n\nCode\nmc3_edges_final &lt;- mc3_edges_indexed %&gt;%\n  left_join(old_to_new_index, \n            by = c(\"from\" = \"old_index\")) %&gt;%\n  rename(from_new = new_index) %&gt;%\n  left_join(old_to_new_index, \n            by = c(\"to\" = \"old_index\")) %&gt;%\n  rename(to_new = new_index) %&gt;%\n  select(from = from_new, to = to_new, \n         is_inferred, type)\n\n\n\n\n\nNow we are ready to build the tidygraph object by using the code chunk below.\n\nmc3_graph &lt;- tbl_graph(\n  nodes = mc3_nodes_final,\n  edges = mc3_edges_final,\n  directed = TRUE\n)\n\nAfter the tidygraph object is created, it is always a good practice to examine the object by using¬†str().\n\nstr(mc3_graph)\n\nClasses 'tbl_graph', 'igraph'  hidden list of 10\n $ : num 1159\n $ : logi TRUE\n $ : num [1:3226] 0 0 0 0 0 0 0 1 1 1 ...\n $ : num [1:3226] 1137 356 746 894 875 ...\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ :List of 4\n  ..$ : num [1:3] 1 0 1\n  ..$ : Named list()\n  ..$ :List of 31\n  .. ..$ type             : chr [1:1159] \"Entity\" \"Entity\" \"Entity\" \"Entity\" ...\n  .. ..$ label            : chr [1:1159] \"Sam\" \"Kelly\" \"Nadia Conti\" \"Elise\" ...\n  .. ..$ name             : chr [1:1159] \"Sam\" \"Kelly\" \"Nadia Conti\" \"Elise\" ...\n  .. ..$ sub_type         : chr [1:1159] \"Person\" \"Person\" \"Person\" \"Person\" ...\n  .. ..$ id               : chr [1:1159] \"Sam\" \"Kelly\" \"Nadia Conti\" \"Elise\" ...\n  .. ..$ timestamp        : chr [1:1159] NA NA NA NA ...\n  .. ..$ monitoring_type  : chr [1:1159] NA NA NA NA ...\n  .. ..$ findings         : chr [1:1159] NA NA NA NA ...\n  .. ..$ content          : chr [1:1159] NA NA NA NA ...\n  .. ..$ assessment_type  : chr [1:1159] NA NA NA NA ...\n  .. ..$ results          : chr [1:1159] NA NA NA NA ...\n  .. ..$ movement_type    : chr [1:1159] NA NA NA NA ...\n  .. ..$ destination      : chr [1:1159] NA NA NA NA ...\n  .. ..$ enforcement_type : chr [1:1159] NA NA NA NA ...\n  .. ..$ outcome          : chr [1:1159] NA NA NA NA ...\n  .. ..$ activity_type    : chr [1:1159] NA NA NA NA ...\n  .. ..$ participants     : int [1:1159] NA NA NA NA NA NA NA NA NA NA ...\n  .. ..$ reference        : chr [1:1159] NA NA NA NA ...\n  .. ..$ date             : chr [1:1159] NA NA NA NA ...\n  .. ..$ time             : chr [1:1159] NA NA NA NA ...\n  .. ..$ friendship_type  : chr [1:1159] NA NA NA NA ...\n  .. ..$ permission_type  : chr [1:1159] NA NA NA NA ...\n  .. ..$ start_date       : chr [1:1159] NA NA NA NA ...\n  .. ..$ end_date         : chr [1:1159] NA NA NA NA ...\n  .. ..$ report_type      : chr [1:1159] NA NA NA NA ...\n  .. ..$ submission_date  : chr [1:1159] NA NA NA NA ...\n  .. ..$ jurisdiction_type: chr [1:1159] NA NA NA NA ...\n  .. ..$ authority_level  : chr [1:1159] NA NA NA NA ...\n  .. ..$ coordination_type: chr [1:1159] NA NA NA NA ...\n  .. ..$ operational_role : chr [1:1159] NA NA NA NA ...\n  .. ..$ new_index        : int [1:1159] 1 2 3 4 5 6 7 8 9 10 ...\n  ..$ :List of 2\n  .. ..$ is_inferred: logi [1:3226] TRUE FALSE TRUE TRUE TRUE TRUE ...\n  .. ..$ type       : chr [1:3226] NA \"sent\" NA NA ...\n $ :&lt;environment: 0x0000023a869743f0&gt; \n - attr(*, \"active\")= chr \"nodes\"\n\n\n\n\n\n\nSeveral of the¬†ggraph¬†layouts involve randomisation. In order to ensure reproducibility, it is necessary to set the seed value before plotting by using the code chunk below.\n\nset.seed(1818)\n\n\n\nShows how many nodes are of type Entity, Event, or Relationship.\n\n\nCode\nmc3_nodes_final %&gt;%\n  count(type, sort = TRUE) %&gt;%\n  ggplot(aes(x = reorder(type, -n), y = n, fill = type)) +\n  geom_col() +\n  geom_text(aes(label = n), vjust = -0.3) +\n  labs(title = \"Node Type Distribution\", x = \"Type\", y = \"Count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nIn the code chunk below,¬†ggraph¬†functions are used to create the whole graph.\n\n\nCode\nggraph(mc3_graph, \n       layout = \"fr\") +\n  geom_edge_link(alpha = 0.3, \n                 colour = \"gray\") +\n  geom_node_point(aes(color = `type`), \n                  size = 4) +\n  geom_node_text(aes(label = type), \n                 repel = TRUE, \n                 size = 2.5) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\nFocuses on what kinds of actors are in the graph ‚Äî Person, Vessel, Organization, etc.\n\n\nCode\n# Define color mapping\nsubtype_colors &lt;- c(\n  \"Person\" = \"#2ca5ff\",\n  \"Organization\" = \"#f5ee15\",\n  \"Vessel\" = \"#FB7E81\",\n  \"Group\" = \"#25e158\",\n  \"Location\" = \"#ec4bff\"\n)\n\nmc3_nodes_final %&gt;%\n  filter(type == \"Entity\") %&gt;%\n  count(sub_type, sort = TRUE) %&gt;%\n  ggplot(aes(x = reorder(sub_type, n), y = n, fill = sub_type)) +\n  geom_col() +\n  coord_flip() +\n  geom_text(aes(label = n), hjust = -0.1) +\n  labs(title = \"Entity Sub-type Distribution\", x = \"Sub-type\", y = \"Count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\nTo understand what kinds of actions dominate ‚Äî Communication, Monitoring, Assessment, etc.\n\n\nCode\nmc3_nodes_final %&gt;%\n  filter(type == \"Event\") %&gt;%\n  count(sub_type, sort = TRUE) %&gt;%\n  ggplot(aes(x = reorder(sub_type, n), y = n, fill = sub_type)) +\n  geom_col() +\n  coord_flip() +\n  geom_text(aes(label = n), hjust = -0.1) +\n  labs(title = \"Event Sub-type Distribution\", x = \"Sub-type\", y = \"Count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\nThis finds all Entities that sent or received communication events ‚Äî i.e., actors who participated in messaging.\n\n\nCode\nlibrary(DT)\n\n# Step 1: Get all Communication Event IDs\ncomm_event_ids &lt;- mc3_nodes_cleaned %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  pull(id)\n\n# Step 2: Extract 'sent' edges for communication events\ncomm_sent_edges &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"sent\", to_id %in% comm_event_ids) %&gt;%\n  select(comm_id = to_id, sender_id = from_id)\n\n# Step 3: Extract 'received' edges for same communication events\ncomm_received_edges &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"received\", from_id %in% comm_event_ids) %&gt;%\n  select(comm_id = from_id, receiver_id = to_id)\n\n# Step 4: Join sent and received edges by communication ID\ncomm_pairs &lt;- comm_sent_edges %&gt;%\n  inner_join(comm_received_edges, by = \"comm_id\")\n\n# Step 5: Add sender and receiver labels\nparticipants_named &lt;- comm_pairs %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, sender_label = label), by = c(\"sender_id\" = \"id\")) %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, receiver_label = label), by = c(\"receiver_id\" = \"id\"))\n\n\n\n# Step7: Interactive summary of top sender‚Äìreceiver pairs\nparticipants_named %&gt;%\n  count(sender_label, receiver_label, sort = TRUE) %&gt;%\n  datatable(\n    caption = \"Top Communication Pairs (Sender ‚Üí Receiver)\",\n    colnames = c(\"Sender\", \"Receiver\", \"Message Count\"),\n    options = list(pageLength = 10, autoWidth = TRUE),\n    rownames = FALSE\n  )\n\n\n\n\n\n\n\n\n\nThis code creates an interactive communication network graph using visNetwork, where:\n\nEach node represents a person or entity, node size is based on total messages sent by that participant.\nEach edge (arrow) represents a communication sent from one participant to another, the thicker the edge, the more message sent to that particular receiver.\n\nVer 1: Layout_in_circle\nMessage Senders are arranged from most to the least number of messages sent.\n\n\nCode\nlibrary(visNetwork)\n\n# Step 1: Summarize communication edges\ncomm_edges_vis &lt;- participants_named %&gt;%\n  count(sender_id, receiver_id, sort = TRUE) %&gt;%\n  rename(from = sender_id, to = receiver_id, value = n)\n\n# Step 2: Compute messages sent per node\nmessage_counts &lt;- comm_edges_vis %&gt;%\n  group_by(from) %&gt;%\n  summarise(sent_count = sum(value), .groups = \"drop\")\n\n# Step 3: Prepare nodes, merge with message count and add color/shape\nnodes_vis &lt;- mc3_nodes_cleaned %&gt;%\n  filter(id %in% unique(c(comm_edges_vis$from, comm_edges_vis$to))) %&gt;%\n  select(id, label, sub_type) %&gt;%\n  left_join(message_counts, by = c(\"id\" = \"from\")) %&gt;%\n  mutate(\n    sent_count = replace_na(sent_count, 0),\n    size = rescale(sent_count, to = c(10, 40)),\n    title = paste0(label, \"&lt;br&gt;Sub-type: \", sub_type,\n                   ifelse(!is.na(sent_count), paste0(\"&lt;br&gt;Sent: \", sent_count, \" messages\"), \"\")),\n    color = case_when(\n      sub_type == \"Person\" ~ \"#2ca5ff\",\n      sub_type == \"Organization\" ~ \"#f5ee15\",\n      sub_type == \"Vessel\" ~ \"#FB7E81\",\n      sub_type == \"Group\" ~ \"#25e158\",\n      sub_type == \"Location\" ~ \"#ec4bff\",\n      TRUE ~ \"black\"\n    ),\n    shape = case_when(\n      sub_type == \"Person\" ~ \"dot\",\n      sub_type == \"Organization\" ~ \"square\",\n      sub_type == \"Vessel\" ~ \"triangle\",\n      sub_type == \"Group\" ~ \"star\",\n      sub_type == \"Location\" ~ \"diamond\",\n      TRUE ~ \"dot\"\n    ),\n  ) %&gt;%\n  arrange(desc(size))\n\n# Step 4: Format visNetwork edges\nedges_vis &lt;- comm_edges_vis %&gt;%\n  mutate(\n    arrows = \"to\",\n    width = rescale(value, to = c(1, 6)),\n    title = paste(\"Messages:\", value)\n  )\n\n# Step 5: Define legend items\nlegend_nodes &lt;- data.frame(\n  label = c(\"Person\", \"Organization\", \"Vessel\", \"Group\", \"Location\"),\n  color = c(\"#2ca5ff\", \"#f5ee15\", \"#FB7E81\", \"#25e158\", \"#ec4bff\"),\n  shape = c(\"dot\", \"square\", \"triangle\", \"star\", \"diamond\"),\n  stringsAsFactors = FALSE\n)\n\n# Step 6: Render network with legend\nvisNetwork(nodes_vis, edges_vis, width = \"100%\", height = \"1000px\") %&gt;%\n  visNodes(\n    size = nodes_vis$size\n    # color and shape are picked up from nodes_vis columns automatically\n  ) %&gt;%\n  visLegend(\n    addNodes = lapply(1:nrow(legend_nodes), function(i) {\n      list(\n        label = legend_nodes$label[i],\n        shape = legend_nodes$shape[i],\n        color = legend_nodes$color[i]\n      )\n    }),\n    useGroups = FALSE,\n    width = 0.15\n  ) %&gt;%\n  visEdges(smooth = FALSE) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visIgraphLayout(layout = \"layout_in_circle\") %&gt;%\n  visPhysics(enabled = FALSE) %&gt;%\n  visLayout(randomSeed = 1818)\n\n\n\n\n\n\nVer 2: Layout_on_sphere\nFrom this plot, it reveals that some pairs (e.g., Miranda Jordan and Clepper Jensen) mainly communicate with each other, suggesting isolated or private channels outside the broader network.\n\n\nCode\nlibrary(visNetwork)\n\n# Step 1: Summarize communication edges\ncomm_edges_vis &lt;- participants_named %&gt;%\n  count(sender_id, receiver_id, sort = TRUE) %&gt;%\n  rename(from = sender_id, to = receiver_id, value = n)\n\n# Step 2: Compute messages sent per person (by sender)\nmessage_counts &lt;- comm_edges_vis %&gt;%\n  group_by(from) %&gt;%\n  summarise(sent_count = sum(value), .groups = \"drop\")\n\n# Step 3: Prepare nodes with label, subtype, color, shape, and scaled size\nnodes_vis &lt;- mc3_nodes_cleaned %&gt;%\n  filter(id %in% unique(c(comm_edges_vis$from, comm_edges_vis$to))) %&gt;%\n  select(id, label, sub_type) %&gt;%\n  left_join(message_counts, by = c(\"id\" = \"from\")) %&gt;%\n  mutate(\n    size = if_else(\n      sub_type == \"Person\",\n      rescale(sent_count, to = c(10, 40), na.rm = TRUE),\n      15\n    ),\n    title = paste0(label, \"&lt;br&gt;Sub-type: \", sub_type,\n                   ifelse(!is.na(sent_count), paste0(\"&lt;br&gt;Sent: \", sent_count, \" messages\"), \"\")),\n    color = case_when(\n      sub_type == \"Person\" ~ \"#2ca5ff\",\n      sub_type == \"Organization\" ~ \"#f5ee15\",\n      sub_type == \"Vessel\" ~ \"#FB7E81\",\n      sub_type == \"Group\" ~ \"#25e158\",\n      sub_type == \"Location\" ~ \"#ec4bff\",\n      TRUE ~ \"black\"\n    ),\n    shape = case_when(\n      sub_type == \"Person\" ~ \"dot\",\n      sub_type == \"Organization\" ~ \"square\",\n      sub_type == \"Vessel\" ~ \"triangle\",\n      sub_type == \"Group\" ~ \"star\",\n      sub_type == \"Location\" ~ \"diamond\",\n      TRUE ~ \"dot\"\n    )\n  )\n\n# Step 4: Format edges\nedges_vis &lt;- comm_edges_vis %&gt;%\n  mutate(\n    arrows = \"to\",\n    width = rescale(value, to = c(1, 6)),\n    title = paste(\"Messages:\", value)\n  )\n\n# Step 5: Legend mapping\nlegend_nodes &lt;- data.frame(\n  label = c(\"Person\", \"Organization\", \"Vessel\", \"Group\", \"Location\"),\n  color = c(\"#2ca5ff\", \"#f5ee15\", \"#FB7E81\", \"#25e158\", \"#ec4bff\"),\n  shape = c(\"dot\", \"square\", \"triangle\", \"star\", \"diamond\"),\n  stringsAsFactors = FALSE\n)\n\n# Step 6: Render the network with layout_on_sphere and legend\nvisNetwork(nodes_vis, edges_vis, width = \"100%\", height = \"900px\") %&gt;%\n  visNodes(\n    size = nodes_vis$size\n    # color and shape columns are automatically used\n  ) %&gt;%\n  visLegend(\n    addNodes = lapply(1:nrow(legend_nodes), function(i) {\n      list(\n        label = legend_nodes$label[i],\n        shape = legend_nodes$shape[i],\n        color = legend_nodes$color[i]\n      )\n    }),\n    useGroups = FALSE,\n    width = 0.15\n  ) %&gt;%\n  visEdges(smooth = FALSE) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visIgraphLayout(layout = \"layout_on_sphere\") %&gt;%\n  visPhysics(enabled = FALSE) %&gt;%\n  visLayout(randomSeed = 1818)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVAST Challenge Task & Question 1a and 1b\n\n\n\nClepper found that messages frequently came in at around the same time each day.\n\nDevelop a graph-based visual analytics approach to identify any daily temporal patterns in communications.\nHow do these patterns shift over the two weeks of observations?\n\n\n\nObjective\n\nIdentify when communications happen most often during each day.\nDetect shifts in these patterns over the 2-week period.\nLater: Focus on a specific entity (e.g., Nadia Conti) and explore who influences them.\n\n\n\nExtract the Communication Timestamps from mc3_nodes_final and filter for communication events.\n\n# Filter for Communication events\ncomm_events &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(timestamp = ymd_hms(timestamp)) %&gt;%\n  filter(!is.na(timestamp)) %&gt;%\n  mutate(\n    day = as.Date(timestamp),\n    hour = hour(timestamp)\n  )\n\nParse the Communication Timestamp into the format ‚Äúdd/mm/yyy (ddd)‚Äù for ease of reference.\n\n# Communication events with parsed date and time\ncomm_events &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(timestamp = ymd_hms(timestamp)) %&gt;%\n  filter(!is.na(timestamp)) %&gt;%\n  mutate(\n    hour = hour(timestamp),\n    date_label = format(timestamp, \"%d/%m/%Y (%a)\")  # e.g., \"19/03/2040 (Tue)\"\n  )\n\n\n\n\n\n\n\n\nCode\n# Step 1: Prepare daily message volume data\ndaily_message_volume &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(\n    timestamp = ymd_hms(timestamp),\n    date = as.Date(timestamp),\n    date_label = format(timestamp, \"%d/%m/%Y (%a)\")\n  ) %&gt;%\n  group_by(date, date_label) %&gt;%\n  summarise(message_count = n(), .groups = \"drop\") %&gt;%\n  arrange(date)\n\n# Step 2: Compute average and total message count\navg_msg_count &lt;- mean(daily_message_volume$message_count)\ntotal_msg_count &lt;- sum(daily_message_volume$message_count)\n\n# Step 3: Plot bar chart with average + total labels\nggplot(daily_message_volume, aes(x = date_label, y = message_count)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  geom_text(\n    aes(label = message_count),\n    vjust = -0.3,\n    size = 2.5,\n    color = \"grey40\"\n  ) +\n  geom_hline(yintercept = avg_msg_count, color = \"red\", linetype = \"dashed\", size = 1.2) +\n  annotate(\n    \"label\", x = 1, y = avg_msg_count + 2,\n    label = paste(\"Average =\", round(avg_msg_count, 1)),\n    color = \"red\", fill = \"grey90\",\n    label.size = 0, hjust = -0.2, vjust = 3\n  ) +\n  annotate(\n    \"label\", x = nrow(daily_message_volume), y = max(daily_message_volume$message_count) + 5,\n    label = paste(\"Total =\", total_msg_count),\n    color = \"black\", fill = \"lightgrey\",\n    label.size = 0.3, hjust = 1.1, vjust = 1\n  ) +\n  labs(\n    title = \"Daily Radio Communication Volume\",\n    x = \"Date\",\n    y = \"Message Count\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(DT)\n\n# Daily message volume with comparisons\ndaily_message_volume &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(\n    timestamp = ymd_hms(timestamp),\n    date = as.Date(timestamp),\n    date_label = format(timestamp, \"%d/%m/%Y (%a)\")\n  ) %&gt;%\n  group_by(date, date_label) %&gt;%\n  summarise(message_count = n(), .groups = \"drop\") %&gt;%\n  arrange(date) %&gt;%\n  mutate(\n    change_from_prev = message_count - lag(message_count),\n    pct_change_from_prev = round((message_count - lag(message_count)) / lag(message_count) * 100, 2)\n  )\n\ndatatable(\n  daily_message_volume %&gt;% select(-date),  # remove raw date if not needed\n  caption = \"Daily Message Volume with Day-over-Day Change\",\n  options = list(pageLength = 14, order = list(list(0, 'asc'))),\n  rownames = FALSE\n)\n\n\n\n\n\n\n\n\n\nThis heat map is interactive and you may choose to hover on the tile to display the date, time, and message count\n\n\nCode\nlibrary(forcats)\nlibrary(plotly)\n\n# Step 1: Reconstruct sender‚Äìreceiver‚Äìtimestamp structure\ncomm_events_raw &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  select(event_id = id, timestamp) %&gt;%\n  mutate(timestamp = ymd_hms(timestamp),\n         hour = hour(timestamp),\n         date_label = format(timestamp, \"%d/%m/%Y (%a)\"))\n\n# Step 2: Get sender (sent) and receiver (received) links\ncomm_edges_sent &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"sent\") %&gt;%\n  select(event_id = to_id, sender_id = from_id)\n\ncomm_edges_recv &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"received\") %&gt;%\n  select(event_id = from_id, receiver_id = to_id)\n\n# Step 3: Join all together into sender‚Äìreceiver‚Äìtimestamp\ncomm_links &lt;- comm_events_raw %&gt;%\n  left_join(comm_edges_sent, by = \"event_id\") %&gt;%\n  left_join(comm_edges_recv, by = \"event_id\") %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(sender_id = id, sender_label = label), by = \"sender_id\") %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(receiver_id = id, receiver_label = label), by = \"receiver_id\")\n\n# Step 4: Aggregate total messages per hour/day\ncomm_heatmap &lt;- comm_links %&gt;%\n  group_by(date_label, hour) %&gt;%\n  summarise(\n    count = n(),\n    top_sender = names(sort(table(sender_label), decreasing = TRUE))[1],\n    sender_count = max(table(sender_label)),\n    top_receiver = names(sort(table(receiver_label), decreasing = TRUE))[1],\n    receiver_count = max(table(receiver_label)),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    tooltip = paste0(\n      \"üìÖ Date: \", date_label,\n      \"&lt;br&gt;‚è∞ Hour: \", sprintf(\"%02d:00\", hour),\n      \"&lt;br&gt;üì® Messages: \", count,\n      \"&lt;br&gt;üî¥ Top Sender: \", top_sender, \" (\", sender_count, \")\",\n      \"&lt;br&gt;üü¢ Top Receiver: \", top_receiver, \" (\", receiver_count, \")\"\n    )\n  )\n\n# Step 5: Static ggplot\np &lt;- ggplot(comm_heatmap, aes(\n  x = hour,\n  y = fct_rev(factor(date_label)),\n  fill = count,\n  text = tooltip\n)) +\n  geom_tile(color = \"white\") +\n  scale_fill_viridis_c(option = \"inferno\", direction = -1, name = \"Message Count\") +\n  scale_x_continuous(\n    breaks = 0:23,\n    labels = function(x) sprintf(\"%02d:00\", x)\n  ) +\n  labs(\n    title = \"Hourly Heatmap of Radio Communications by Day\",\n    x = \"Hour of Day\",\n    y = NULL\n  ) +\n  theme_minimal(base_size = 10) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid = element_blank()\n  )\n\n# Step 6: Make interactive\nggplotly(p, tooltip = \"text\")\n\n\n\n\n\n\nWe will increase the resolution to half-hour time slots.\n\n\n\nThis heat map is interactive and you may choose to hover on the tile to display the date, time, and message count.\n\n\nCode\nlibrary(forcats)\nlibrary(plotly)\n\n# Step 1: Fix sender and receiver edges\ncomm_edges_sent &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"sent\") %&gt;%\n  select(event_id = to_id, sender_id = from_id)\n\ncomm_edges_recv &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"received\") %&gt;%\n  select(event_id = from_id, receiver_id = to_id)  # ‚úÖ fixed receiver_id\n\n# Step 2: Reconstruct sender‚Äìreceiver‚Äìevent linkage\ncomm_events_raw &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  select(event_id = id, timestamp) %&gt;%\n  mutate(\n    timestamp = ymd_hms(timestamp),\n    hour = hour(timestamp),\n    minute = minute(timestamp),\n    time_bin = hour + ifelse(minute &lt; 30, 0, 0.5),\n    date_label = format(timestamp, \"%d/%m/%Y (%a)\"),\n    time_label = sprintf(\"%02d:%02d\", floor(time_bin), ifelse(time_bin %% 1 == 0, 0, 30))\n  )\n\n# Step 3: Join to get sender/receiver labels\ncomm_links &lt;- comm_events_raw %&gt;%\n  left_join(comm_edges_sent, by = \"event_id\") %&gt;%\n  left_join(comm_edges_recv, by = \"event_id\") %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, sender_label = label), by = c(\"sender_id\" = \"id\")) %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, receiver_label = label), by = c(\"receiver_id\" = \"id\"))\n\n# Step 4: Aggregate by half-hour + label top actors\ncomm_heatmap &lt;- comm_links %&gt;%\n  group_by(date_label, time_bin, time_label) %&gt;%\n  summarise(\n    count = n(),\n    top_sender = names(sort(table(sender_label), decreasing = TRUE))[1],\n    sender_count = max(table(sender_label)),\n    top_receiver = names(sort(table(receiver_label), decreasing = TRUE))[1],\n    receiver_count = max(table(receiver_label)),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    tooltip = paste0(\n      \"üìÖ Date: \", date_label,\n      \"&lt;br&gt;üïí Time: \", time_label,\n      \"&lt;br&gt;üì® Messages: \", count,\n      \"&lt;br&gt;üî¥ Top Sender: \", top_sender, \" (\", sender_count, \")\",\n      \"&lt;br&gt;üü¢ Top Receiver: \", top_receiver, \" (\", receiver_count, \")\"\n    )\n  )\n\n# Step 5: ggplot\np &lt;- ggplot(comm_heatmap, aes(x = time_bin, y = fct_rev(factor(date_label)), fill = count, text = tooltip)) +\n  geom_tile(color = \"white\") +\n  scale_fill_viridis_c(\n    option = \"inferno\",\n    direction = -1,\n    name = \"Message Count\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  labs(\n    title = \"Half-Hourly Heatmap of Radio Communications by Day\",\n    x = \"Time of Day\",\n    y = NULL\n  ) +\n  theme_minimal(base_size = 10) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid = element_blank()\n  )\n\n# Step 6: Convert to interactive Plotly plot\nggplotly(p, tooltip = \"text\")\n\n\n\n\n\n\n\n\n\nThe faceted density plot that shows the distribution of communication events by time of day, broken down for each day in the dataset. It helps to visually detect temporal communication patterns, intensity, and consistency over multiple days.\n\nOverview of the 2 week periodDay 1 - 01/10/2040Day 2 - 02/10/2040Day 3 - 03/10/2040Day 4 - 04/10/2040Day 5 - 05/10/2040Day 6 - 06/10/2040Day 7 - 07/10/2040Day 8 - 08/10/2040Day 9 - 09/10/2040Day 10 - 10/10/2040Day 11 - 11/10/2040Day 12 - 12/10/2040Day 13 - 13/10/2040Day 14 - 14/10/2040\n\n\n\n\nCode\n# Step 1: Preprocess communication events\ncomm_events &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(timestamp = ymd_hms(timestamp)) %&gt;%\n  filter(!is.na(timestamp)) %&gt;%\n  mutate(\n    date_label = format(timestamp, \"%d/%m/%Y (%a)\"),\n    hour = hour(timestamp),\n    minute = minute(timestamp),\n    time_bin = hour + ifelse(minute &lt; 30, 0, 0.5)\n  )\n\n# Step 2: Summarise daily medians and counts\ndaily_stats &lt;- comm_events %&gt;%\n  group_by(date_label) %&gt;%\n  summarise(\n    median_time = median(time_bin),\n    msg_count = n(),\n    .groups = \"drop\"\n  )\n\n# Step 3: Plot\nggplot(comm_events, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = daily_stats, aes(xintercept = median_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(\n    data = daily_stats,\n    aes(x = 20.5, y = 0.25, label = paste(\"Total:\", msg_count)),\n    inherit.aes = FALSE,\n    size = 3,\n    color = \"grey20\",\n    hjust = 1\n  ) +\n  facet_wrap(~ date_label, ncol = 4) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = NULL  # suppress all x-axis labels\n  ) +\n  labs(\n    title = \"Daily Communication Patterns (Half-Hourly)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"01/10/2040 (Mon)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"02/10/2040 (Tue)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"03/10/2040 (Wed)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"04/10/2040 (Thu)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"05/10/2040 (Fri)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"06/10/2040 (Sat)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"07/10/2040 (Sun)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"08/10/2040 (Mon)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"09/10/2040 (Tue)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"10/10/2040 (Wed)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"11/10/2040 (Thu)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"12/10/2040 (Fri)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"13/10/2040 (Sat)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"14/10/2040 (Sun)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nüìà Insights This Visualization Offers\n\n\n\n\nBar Plot of combined hourly message volume over the 2 weeks period:\n\n\nCode\n# Prepare data\ncomm_hourly &lt;- comm_events %&gt;%\n  count(hour) %&gt;%\n  mutate(\n    hour_label = sprintf(\"%02d:00\", hour),  # Format to hh:mm\n    percent = n / sum(n)\n  )\n\n# Plot\nggplot(comm_hourly, aes(x = hour_label, y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  geom_text_repel(\n    aes(label = paste0(n, \" (\", percent(percent, accuracy = 1), \")\")),\n    nudge_y = 3,\n    size = 2.5,\n    direction = \"y\",\n    max.overlaps = Inf\n  ) +\n  labs(\n    title = \"Overall Hourly Communication Volume\",\n    x = \"Time of Day (hh:mm)\",\n    y = \"Message Count\"\n  ) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nBar Plot of combined half-hourly message volume in the 2 weeks period.\n\n\nCode\ncomm_events &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(timestamp = ymd_hms(timestamp)) %&gt;%\n  filter(!is.na(timestamp)) %&gt;%\n  mutate(\n    hour = hour(timestamp),\n    minute = minute(timestamp),\n    time_bin = sprintf(\"%02d:%02d\", hour, ifelse(minute &lt; 30, 0, 30))\n  )\n\ncomm_halfhour &lt;- comm_events %&gt;%\n  count(time_bin) %&gt;%\n  mutate(percent = n / sum(n))\n\nggplot(comm_halfhour, aes(x = time_bin, y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  geom_text_repel(\n    aes(label = paste0(n, \" (\", percent(percent, accuracy = 1), \")\")),\n    nudge_y = 3,\n    size = 2.5,\n    direction = \"y\",\n    max.overlaps = Inf\n  ) +\n  labs(\n    title = \"Overall Half-Hourly Communication Volume\",\n    x = \"Time of Day (hh:mm)\",\n    y = \"Message Count\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1a. What are the identifiable daily temporal patterns in communications?\n\n\n\n\nThe daily communication volume fluctuates slightly between 34 and 49 messages, with an average of approximately 42 messages per day, highlighting a stable overall activity level. Notably, the highest volume occurs on 11th October (49 messages), immediately following the lowest volume the day before on 10th October (34 messages)‚Äîa sharp rebound that may signal a response to specific events or operational needs. Despite these fluctuations, the system maintains a consistent tempo across the two weeks.\nThe temporal analysis using both the heat map and time series plots reveals a pronounced morning-centric communication rhythm. The vast majority of radio traffic is concentrated between 9:00 AM and 11:30 AM, with the most intense peaks typically occurring between 10:00 and 11:00 AM. With reference to the Density plot of Daily half-hourly message volume, of the 14 days, we see message density peaks at 10:30 AM on 9 days, while on 3 days, it peaks at 12:30 PM.\nFor instance if we were to based in on the hourly plot, 5th October (Fri) and 11th October (Thu) both register their highest single-hour counts at 10:00 AM at 24 and 21 messages respectively. Communication activity drops off steeply after lunchtime, with more than 90% of the days showing little to no activity after 2:30 PM. This pattern suggests a highly structured daily workflow, where key decisions and coordination are front-loaded in the day. Importantly, the hourly heat map also indicates that this routine holds across both weekdays and weekends‚Äîcommunication volumes and peak hours remain similar, underlining the operational regularity of the group regardless of the day of week.\n\n\n\n\n\n\n\n\n\n1b. How do these patterns shift over the two weeks of observations?\n\n\n\n\nOver the two-week period, while the timing and structure of communication peaks remain broadly consistent, there are subtle shifts in both intensity and timing. Some days, such as 3rd, 5th, 11th and 12th October, see particularly high spikes in the mid-morning, which may correspond to critical events, decision points, or heightened urgency. The sharp dip on October 8th and 13th, immediately after a period of ‚Äúsurge‚Äù (3rd - 7th and 9th to 12th October), points to possible responses to interruptions, lulls, or triggering incidents. Overall, although the daily messaging routine is remarkably stable, these bursts and brief lulls provide clues to changing circumstances or stress points in the operation‚Äîan analytical signal that warrants closer inspection of event logs or external triggers for those dates.\nAnother notable change in the communication pattern is observed during the weekends. In the first week, weekend communication peaks occurred earlier, typically between 10:00 AM and 11:30 AM, closely mirroring the weekday rhythm. However, in the second week, the weekend peaks shifted noticeably later, with the highest message volumes concentrated around 12:00 PM and 1:00 PM. This shift not only marks a departure from the otherwise stable early-morning communication structure but also suggests an adaptive or reactive operational schedule‚Äîpotentially in response to evolving events, increased coordination needs, or changing priorities as the observation period progressed. The contrast between the two weekends is clear in the heatmap, underscoring the importance of monitoring such shifts as possible indicators of underlying changes in group behavior or external pressures.\n\n\n\n\n\n\n\n\n\n\n\n\n\nVAST Challenge Task & Question 1c\n\n\n\nClepper found that messages frequently came in at around the same time each day.\n\nFocus on a specific entity and use this information to determine who has influence over them.\n\n\n\n\n\nWe first extracted the relevant communication edges from the dataset, pairing ‚Äúsent‚Äù and ‚Äúreceived‚Äù communication events to form entity-to-entity links. We retained only those edges where both nodes represent real-world entities (Person, Organization, Vessel, Group, or Location), ensuring that our analysis focuses on the meaningful actors in the Oceanus network.\n\n\nCode\n# Extract sent and received communication event edges\nsent_edges &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"sent\") %&gt;%\n  select(source_entity = from_id, event = to_id)\n\nreceived_edges &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"received\") %&gt;%\n  select(event = from_id, target_entity = to_id)\n\n# Pair sent and received to form communication edges\npaired_edges &lt;- sent_edges %&gt;%\n  inner_join(received_edges, by = \"event\") %&gt;%\n  select(from = source_entity, to = target_entity)\n\n# Add unmatched sent and received edges (optional, for completeness)\nsingle_sent_edges &lt;- sent_edges %&gt;%\n  select(from = source_entity, to = event)\nsingle_received_edges &lt;- received_edges %&gt;%\n  select(from = event, to = target_entity)\n\nall_edges &lt;- bind_rows(paired_edges, single_sent_edges, single_received_edges) %&gt;%\n  distinct()\n\n# Identify entity nodes (Person, Organization, Vessel, Group, Location)\nentity_ids &lt;- mc3_nodes_cleaned %&gt;%\n  filter(sub_type %in% c(\"Person\", \"Organization\", \"Vessel\", \"Group\", \"Location\")) %&gt;%\n  pull(id) %&gt;% as.character()\n\nentity_edges &lt;- all_edges %&gt;%\n  filter(as.character(from) %in% entity_ids, as.character(to) %in% entity_ids)\n\nentity_nodes &lt;- mc3_nodes_cleaned %&gt;%\n  filter(sub_type %in% c(\"Person\", \"Organization\", \"Vessel\", \"Group\", \"Location\")) %&gt;%\n  select(id, label, sub_type)\n\n\n\n\n\nUsing these cleaned and filtered edges and nodes, we built a global directed graph representing the Oceanus community. We then computed key network centrality metrics for each node‚ÄîPageRank, betweenness, and degree‚Äîquantifying the influence and connectivity of every entity in the overall network.\n\n\nCode\nlibrary(igraph)\n\ng &lt;- graph_from_data_frame(d = entity_edges, vertices = entity_nodes, directed = TRUE)\n\n# Compute centralities\nV(g)$pagerank &lt;- page_rank(g)$vector\nV(g)$betweenness &lt;- betweenness(g)\nV(g)$degree &lt;- degree(g)\n\n\n\n\n\nFocusing on ‚ÄúNadia Conti‚Äù, we identified her node and extracted her two-step ego network, capturing both direct and indirect connections within the broader network. This local subgraph reveals Nadia‚Äôs immediate sphere of influence and the key players connected to her.\n\n\nCode\nnadia_label &lt;- \"Nadia Conti\"\ntarget_index &lt;- which(V(g)$label == nadia_label)\n\nego_graph &lt;- make_ego_graph(g, order = 2, nodes = target_index, mode = \"all\")[[1]]\n\n\n\n\n\nWe visualized Nadia‚Äôs ego network using node size, shape, and color to represent centrality and entity type. We also summarized centrality metrics in clear tables, ranking all ego network members by PageRank, Betweenness, and Degree. This allows for direct identification of the most influential, best-connected, and most strategic actors in Nadia Conti‚Äôs communication environment.\n\n\nCode\nnodes_df &lt;- data.frame(\n  id = V(ego_graph)$name,\n  label = V(ego_graph)$label,\n  group = V(ego_graph)$sub_type,\n  title = paste0(\"&lt;b&gt;\", V(ego_graph)$label, \"&lt;/b&gt;&lt;br&gt;\",\n                 \"Degree: \", round(V(ego_graph)$degree, 2), \"&lt;br&gt;\",\n                 \"Betweenness: \", round(V(ego_graph)$betweenness, 2), \"&lt;br&gt;\",\n                 \"PageRank: \", round(V(ego_graph)$pagerank, 4)),\n  shape = ifelse(V(ego_graph)$sub_type == \"Person\", \"dot\",\n                 ifelse(V(ego_graph)$sub_type == \"Organization\", \"square\",\n                        ifelse(V(ego_graph)$sub_type == \"Vessel\", \"triangle\",\n                               ifelse(V(ego_graph)$sub_type == \"Group\", \"star\", \"diamond\")))),\n  value = V(ego_graph)$pagerank * 30 + 5\n)\n\nedges_df &lt;- as_data_frame(ego_graph, what = \"edges\") %&gt;%\n  rename(from = from, to = to)\n\nlibrary(visNetwork)\nvisNetwork(nodes_df, edges_df, width = \"100%\", height = \"700px\") %&gt;%\n  visNodes(scaling = list(min = 5, max = 30)) %&gt;%\n  visEdges(\n    arrows = list(to = list(enabled = TRUE, scaleFactor = 0.3)),\n    color = list(color = \"gray\")\n  ) %&gt;%\n  visOptions(\n    highlightNearest = TRUE,\n    nodesIdSelection = TRUE,\n    manipulation = FALSE\n  ) %&gt;%\n  visInteraction(\n    dragNodes = FALSE,\n    dragView = FALSE,\n    zoomView = FALSE\n  ) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 1818)\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal and Ego-Network Structure\n\n\n\nThe overview network visualization reveals that Nadia Conti is centrally embedded in the Oceanus communication web, maintaining direct and indirect connections with major actors such as Neptune (Vessel), V. Miesel Shipping (Organization), Elise (Person), and others. The use of color and shape coding in the network allows for quick identification of the different types of entities in Nadia‚Äôs influence neighborhood.\n\n\n\n\n\nOn both the global and Nadia-focused ego networks, we computed standard network centrality metrics for all nodes:\n\nPageRank (overall influence),\nBetweenness (information brokerage/intermediary role), and\nDegree (number of direct connections).\n\nThese measures quantify the importance and structural roles of each entity relative to Nadia and the broader community.\n\n\nCode\n# PageRank table\npagerank_df &lt;- data.frame(\n  label = V(ego_graph)$label,\n  sub_type = V(ego_graph)$sub_type,\n  pagerank = round(V(ego_graph)$pagerank, 4)\n) %&gt;% arrange(desc(pagerank))\n\n# Betweenness table\nbetweenness_df &lt;- data.frame(\n  label = V(ego_graph)$label,\n  sub_type = V(ego_graph)$sub_type,\n  betweenness = round(V(ego_graph)$betweenness, 2)\n) %&gt;% arrange(desc(betweenness))\n\n# Degree table\ndegree_df &lt;- data.frame(\n  label = V(ego_graph)$label,\n  sub_type = V(ego_graph)$sub_type,\n  degree = V(ego_graph)$degree\n) %&gt;% arrange(desc(degree))\n\n\n\nknitr::kable(pagerank_df, caption = \"PageRank Centrality (Nadia's Ego Network)\")\n\n\nPageRank Centrality (Nadia‚Äôs Ego Network)\n\n\nlabel\nsub_type\npagerank\n\n\n\n\nMako\nVessel\n0.0687\n\n\nOceanus City Council\nOrganization\n0.0530\n\n\nReef Guardian\nVessel\n0.0454\n\n\nNadia Conti\nPerson\n0.0432\n\n\nRemora\nVessel\n0.0409\n\n\nV. Miesel Shipping\nOrganization\n0.0394\n\n\nNeptune\nVessel\n0.0358\n\n\nHimark Harbor\nLocation\n0.0358\n\n\nLiam Thorne\nPerson\n0.0275\n\n\nBoss\nPerson\n0.0272\n\n\nSentinel\nVessel\n0.0250\n\n\nPaackland Harbor\nLocation\n0.0244\n\n\nDavis\nPerson\n0.0239\n\n\nMarlin\nVessel\n0.0235\n\n\nEcoVigil\nVessel\n0.0233\n\n\nGreen Guardians\nOrganization\n0.0224\n\n\nMrs.¬†Money\nPerson\n0.0192\n\n\nSailor Shifts Team\nOrganization\n0.0186\n\n\nSeawatch\nVessel\n0.0186\n\n\nElise\nPerson\n0.0182\n\n\nSerenity\nVessel\n0.0170\n\n\nHorizon\nVessel\n0.0152\n\n\nThe Middleman\nPerson\n0.0142\n\n\nNorthern Light\nVessel\n0.0135\n\n\nRodriguez\nPerson\n0.0122\n\n\nSamantha Blake\nPerson\n0.0114\n\n\nHaacklee Harbor\nLocation\n0.0111\n\n\nOsprey\nVessel\n0.0088\n\n\nCity Officials\nGroup\n0.0066\n\n\nThe Lookout\nPerson\n0.0062\n\n\nKnowles\nVessel\n0.0051\n\n\nSmall Fry\nPerson\n0.0035\n\n\nGlitters Team\nOrganization\n0.0035\n\n\n\n\n\n\nknitr::kable(betweenness_df, caption = \"Betweenness Centrality (Nadia's Ego Network)\")\n\n\nBetweenness Centrality (Nadia‚Äôs Ego Network)\n\n\nlabel\nsub_type\nbetweenness\n\n\n\n\nMako\nVessel\n368.50\n\n\nMrs.¬†Money\nPerson\n167.18\n\n\nReef Guardian\nVessel\n139.69\n\n\nBoss\nPerson\n136.18\n\n\nV. Miesel Shipping\nOrganization\n118.70\n\n\nNadia Conti\nPerson\n117.87\n\n\nOceanus City Council\nOrganization\n116.11\n\n\nRemora\nVessel\n90.45\n\n\nNeptune\nVessel\n82.59\n\n\nThe Lookout\nPerson\n80.51\n\n\nHimark Harbor\nLocation\n52.61\n\n\nThe Middleman\nPerson\n50.78\n\n\nLiam Thorne\nPerson\n41.81\n\n\nHaacklee Harbor\nLocation\n41.30\n\n\nSentinel\nVessel\n34.54\n\n\nGreen Guardians\nOrganization\n27.51\n\n\nPaackland Harbor\nLocation\n27.08\n\n\nDavis\nPerson\n22.36\n\n\nEcoVigil\nVessel\n12.63\n\n\nRodriguez\nPerson\n11.75\n\n\nNorthern Light\nVessel\n9.76\n\n\nSailor Shifts Team\nOrganization\n7.34\n\n\nHorizon\nVessel\n6.72\n\n\nMarlin\nVessel\n6.23\n\n\nSeawatch\nVessel\n5.20\n\n\nElise\nPerson\n4.60\n\n\nSamantha Blake\nPerson\n4.49\n\n\nSerenity\nVessel\n0.81\n\n\nKnowles\nVessel\n0.50\n\n\nSmall Fry\nPerson\n0.00\n\n\nGlitters Team\nOrganization\n0.00\n\n\nOsprey\nVessel\n0.00\n\n\nCity Officials\nGroup\n0.00\n\n\n\n\n\n\nknitr::kable(degree_df, caption = \"Degree Centrality (Nadia's Ego Network)\")\n\n\nDegree Centrality (Nadia‚Äôs Ego Network)\n\n\nlabel\nsub_type\ndegree\n\n\n\n\nMako\nVessel\n37\n\n\nOceanus City Council\nOrganization\n28\n\n\nReef Guardian\nVessel\n27\n\n\nRemora\nVessel\n21\n\n\nV. Miesel Shipping\nOrganization\n19\n\n\nNeptune\nVessel\n19\n\n\nNadia Conti\nPerson\n17\n\n\nGreen Guardians\nOrganization\n17\n\n\nHimark Harbor\nLocation\n17\n\n\nDavis\nPerson\n16\n\n\nSentinel\nVessel\n16\n\n\nBoss\nPerson\n13\n\n\nEcoVigil\nVessel\n13\n\n\nPaackland Harbor\nLocation\n13\n\n\nMrs.¬†Money\nPerson\n12\n\n\nHorizon\nVessel\n12\n\n\nLiam Thorne\nPerson\n11\n\n\nRodriguez\nPerson\n10\n\n\nMarlin\nVessel\n10\n\n\nSeawatch\nVessel\n9\n\n\nThe Middleman\nPerson\n8\n\n\nSerenity\nVessel\n8\n\n\nNorthern Light\nVessel\n8\n\n\nHaacklee Harbor\nLocation\n8\n\n\nElise\nPerson\n7\n\n\nThe Lookout\nPerson\n7\n\n\nSailor Shifts Team\nOrganization\n7\n\n\nSamantha Blake\nPerson\n6\n\n\nGlitters Team\nOrganization\n4\n\n\nKnowles\nVessel\n4\n\n\nSmall Fry\nPerson\n3\n\n\nOsprey\nVessel\n3\n\n\nCity Officials\nGroup\n1\n\n\n\n\n\n\n\n\n\n\n\nCentrality Metrics and Direct & Indirect Influences\n\n\n\nBy calculating centrality metrics within Nadia‚Äôs two-hop ego network, we observe that the most influential nodes in her environment‚Äîby PageRank, betweenness, and degree‚Äîare Neptune, V. Miesel Shipping, and Elise. Nadia herself consistently ranks among the top nodes by these measures, highlighting her role as both an influencer and an information bridge. Entities such as Neptune and V. Miesel Shipping, which also score highly in centrality, exert considerable influence over Nadia‚Äôs information flow and access to other parts of the network.\nDegree centrality analysis shows Nadia maintains multiple direct connections, particularly with other highly active nodes, ensuring she is closely linked to key hubs in the network. Betweenness centrality further reveals that Nadia is not only well-connected but also acts as an important intermediary, facilitating communication between otherwise distant parts of the network. PageRank confirms that her immediate environment is composed of actors with significant structural power, increasing the likelihood that Nadia is both influenced by, and exerts influence upon, the most pivotal players in Oceanus.\n\n\n\n\n\n\nCode\nlibrary(igraph)\nlibrary(visNetwork)\n\n# -- Build the global network g as in your earlier code (using your entity_nodes/entity_edges) --\n\ng &lt;- graph_from_data_frame(\n  d = entity_edges, \n  vertices = entity_nodes, \n  directed = TRUE\n)\n\n# -- Get Nadia's index in g --\nnadia_label &lt;- \"Nadia Conti\"\ntarget_index &lt;- which(V(g)$label == nadia_label)\n\n# -- Extract Nadia's 1-hop ego network (all direct neighbors) --\nego_1 &lt;- make_ego_graph(g, order = 1, nodes = target_index, mode = \"all\")[[1]]\n\n\n# 1. Compute PageRank for the ego network\nV(ego_1)$pagerank &lt;- page_rank(ego_1)$vector\n\n# 2. Prepare node data frame with your consistent color scheme\nnodes_df_pagerank &lt;- data.frame(\n  id = V(ego_1)$name,\n  label = V(ego_1)$label,\n  group = V(ego_1)$sub_type,\n  title = paste0(\"&lt;b&gt;\", V(ego_1)$label, \"&lt;/b&gt;&lt;br&gt;PageRank: \", round(V(ego_1)$pagerank, 4)),\n  shape = ifelse(V(ego_1)$sub_type == \"Person\", \"dot\",\n                 ifelse(V(ego_1)$sub_type == \"Organization\", \"square\",\n                        ifelse(V(ego_1)$sub_type == \"Vessel\", \"triangle\",\n                               ifelse(V(ego_1)$sub_type == \"Group\", \"star\", \"diamond\")))),\n  color = case_when(\n    V(ego_1)$sub_type == \"Person\" ~ \"#2ca5ff\",\n    V(ego_1)$sub_type == \"Organization\" ~ \"#f5ee15\",\n    V(ego_1)$sub_type == \"Vessel\" ~ \"#FB7E81\",\n    V(ego_1)$sub_type == \"Group\" ~ \"#25e158\",\n    V(ego_1)$sub_type == \"Location\" ~ \"#ec4bff\",\n    TRUE ~ \"black\"\n  ),\n  value = V(ego_1)$pagerank * 30 + 5\n)\n\n# 3. Prepare edges\nedges_df &lt;- as_data_frame(ego_1, what = \"edges\") %&gt;%\n  rename(from = from, to = to)\n\n# 4. Plot with visNetwork\nvisNetwork(nodes_df_pagerank, edges_df, width = \"100%\", height = \"400px\") %&gt;%\n  visNodes(\n    scaling = list(min = 5, max = 30),\n    color = list(background = nodes_df_pagerank$color, border = \"black\"),\n    shape = nodes_df_pagerank$shape\n  ) %&gt;%\n  visEdges(arrows = list(to = list(enabled = TRUE, scaleFactor = 0.3)), color = list(color = \"gray\")) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE, manipulation = FALSE) %&gt;%\n  visInteraction(dragNodes = FALSE, dragView = FALSE, zoomView = FALSE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 1818)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 1. Compute Betweenness for the ego network\nV(ego_1)$betweenness &lt;- betweenness(ego_1, directed = TRUE)\n\n# 2. Prepare node data frame\nnodes_df_betweenness &lt;- data.frame(\n  id = V(ego_1)$name,\n  label = V(ego_1)$label,\n  group = V(ego_1)$sub_type,\n  title = paste0(\"&lt;b&gt;\", V(ego_1)$label, \"&lt;/b&gt;&lt;br&gt;Betweenness: \", round(V(ego_1)$betweenness, 2)),\n  shape = ifelse(V(ego_1)$sub_type == \"Person\", \"dot\",\n                 ifelse(V(ego_1)$sub_type == \"Organization\", \"square\",\n                        ifelse(V(ego_1)$sub_type == \"Vessel\", \"triangle\",\n                               ifelse(V(ego_1)$sub_type == \"Group\", \"star\", \"diamond\")))),\n  color = case_when(\n    V(ego_1)$sub_type == \"Person\" ~ \"#2ca5ff\",\n    V(ego_1)$sub_type == \"Organization\" ~ \"#f5ee15\",\n    V(ego_1)$sub_type == \"Vessel\" ~ \"#FB7E81\",\n    V(ego_1)$sub_type == \"Group\" ~ \"#25e158\",\n    V(ego_1)$sub_type == \"Location\" ~ \"#ec4bff\",\n    TRUE ~ \"black\"\n  ),\n  value = V(ego_1)$betweenness * 2 + 5\n)\n\n# 3. Edges (same as before)\n# edges_df already prepared\n\n# 4. Plot\nvisNetwork(nodes_df_betweenness, edges_df, width = \"100%\", height = \"400px\") %&gt;%\n  visNodes(\n    scaling = list(min = 5, max = 30),\n    color = list(background = nodes_df_betweenness$color, border = \"black\"),\n    shape = nodes_df_betweenness$shape\n  ) %&gt;%\n  visEdges(arrows = list(to = list(enabled = TRUE, scaleFactor = 0.3)), color = list(color = \"gray\")) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE, manipulation = FALSE) %&gt;%\n  visInteraction(dragNodes = FALSE, dragView = FALSE, zoomView = FALSE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 1818)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 1. Compute Degree for the ego network\nV(ego_1)$degree &lt;- degree(ego_1, mode = \"all\")\n\n# 2. Prepare node data frame\nnodes_df_degree &lt;- data.frame(\n  id = V(ego_1)$name,\n  label = V(ego_1)$label,\n  group = V(ego_1)$sub_type,\n  title = paste0(\"&lt;b&gt;\", V(ego_1)$label, \"&lt;/b&gt;&lt;br&gt;Degree: \", round(V(ego_1)$degree, 2)),\n  shape = ifelse(V(ego_1)$sub_type == \"Person\", \"dot\",\n                 ifelse(V(ego_1)$sub_type == \"Organization\", \"square\",\n                        ifelse(V(ego_1)$sub_type == \"Vessel\", \"triangle\",\n                               ifelse(V(ego_1)$sub_type == \"Group\", \"star\", \"diamond\")))),\n  color = case_when(\n    V(ego_1)$sub_type == \"Person\" ~ \"#2ca5ff\",\n    V(ego_1)$sub_type == \"Organization\" ~ \"#f5ee15\",\n    V(ego_1)$sub_type == \"Vessel\" ~ \"#FB7E81\",\n    V(ego_1)$sub_type == \"Group\" ~ \"#25e158\",\n    V(ego_1)$sub_type == \"Location\" ~ \"#ec4bff\",\n    TRUE ~ \"black\"\n  ),\n  value = V(ego_1)$degree * 5 + 5\n)\n\n# 3. Edges (same as before)\n# edges_df already prepared\n\n# 4. Plot\nvisNetwork(nodes_df_degree, edges_df, width = \"100%\", height = \"400px\") %&gt;%\n  visNodes(\n    scaling = list(min = 5, max = 30),\n    color = list(background = nodes_df_degree$color, border = \"black\"),\n    shape = nodes_df_degree$shape\n  ) %&gt;%\n  visEdges(arrows = list(to = list(enabled = TRUE, scaleFactor = 0.3)), color = list(color = \"gray\")) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE, manipulation = FALSE) %&gt;%\n  visInteraction(dragNodes = FALSE, dragView = FALSE, zoomView = FALSE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 1818)\n\n\n\n\n\n\n\n\n\n\n\n\n1c. With a focus on ‚ÄúNadia Conti‚Äù, the visuals above could determine who has influence over this person.\n\n\n\n\nDegree centrality reveals that Nadia Conti is well-connected within her local network, with a degree of 17. However, she is not the most connected node; vessels such as Mako (37), Reef Guardian (27), and Remora (21), as well as organizations like Oceanus City Council (28) and V. Miesel Shipping (19), have even higher degrees. This indicates that while Nadia is an important hub, her sphere of direct interaction is embedded within a dense mesh of other highly connected entities.\nSeveral other individuals (e.g., Davis with 16, Boss with 13, Mrs.¬†Money with 12) and vessels (e.g., Neptune with 19, Sentinel with 16) also play significant roles in Nadia‚Äôs network. The presence of organizations (e.g., Green Guardians, Sailor Shifts Team), multiple vessels, and key persons shows that Nadia‚Äôs environment is both diverse and robust.\nDirect Connections\nThese direct connections are clearly shown as nodes that have edges (arrows) going into or out of Nadia Conti‚Äôs node in the network diagrams. Nadia Conti directly connects to several core entities across different types:\n\nPeople: Elise, Liam Thorne, Davis, Rodriguez\nOrganization: V. Miesel Shipping, Oceanus City Council, Sailor Shifts Team\nVessel: Neptune, Marlin, Remora, Sentinel\nLocation: Haacklee Harbor\n\nInterpretation: The PageRank, Betweenness, and Degree centrality plots all consistently show Nadia Conti as a major hub, with a large node size reflecting her high centrality. Her immediate network includes influential vessels (Neptune, Remora), organizations (V. Miesel Shipping, Oceanus City Council), and several persons (Elise, Davis, Rodriguez).\nNadia‚Äôs position suggests she is a key connector and influencer but is herself surrounded by even larger hubs, particularly among vessels and organizations. Her ability to influence‚Äîand be influenced‚Äîis amplified by these connections, as these high-degree entities are likely sources and conduits of critical information and operational coordination. This structure points to a tightly interwoven community, where central actors such as Mako, Oceanus City Council, and V. Miesel Shipping may exert the most substantial influence over Nadia‚Äôs access to information, resources, and strategic decisions."
  },
  {
    "objectID": "TH3/Take-Home_Ex02_MC3.html#getting-started",
    "href": "TH3/Take-Home_Ex02_MC3.html#getting-started",
    "title": "Take-Home Assignment 2",
    "section": "",
    "text": "For the purpose of this assignment, five R packages will be used. They are tidyverse, jsonlite, tidygraph, ggraph and SmartEDA.\nIn the code chunk below, p_load() of pacman package is used to load the R packages into R environment.\n\npacman::p_load(tidyverse, jsonlite, \n               tidygraph, ggraph, SmartEDA, \n               ggrepel, scales, lubridate, dplyr, viridis)\n\n\n\n\nFor the purpose of this exercise, mc3.json file will be used. Before getting started, you should have the data set in the data sub-folder.\nIn the code chunk below, fromJSON() of jsonlite package is used to import mc3.json file into R and save the output object\n\nMC3 &lt;- fromJSON(\"data/MC3_graph.json\")\nMC3_schema &lt;- fromJSON(\"data/MC3_schema.json\")\n\n\n\n\nThe dataset was provided by VAST Challenge for MC3. This report utilizes two core datasets: MC3_graph.json, which encodes the knowledge graph of communications, events, and relationships; and MC3_schema.json, which defines the structure, subtypes, and attributes of each node and edge type within the graph. There ngraph contains a total of 1159 nodes and 3226 edges. Full description of node attributes and edge attributes is shown below.\nNodes Attributes are as such:\n\n\n\nNode Subtypes\n\n\nEdge Attributes are as such:\n\n\n\nNode-Edge-Node Matrix\n\n\n\n\n\nBefore preparing the data, it is always a good practice to examine the structure of mc3 knowledge graph.\nIn the code chunk below glimpse() is used to reveal the structure of mc3 knowledge graph.\n\nglimpse(MC3)\n\nList of 5\n $ directed  : logi TRUE\n $ multigraph: logi FALSE\n $ graph     :List of 4\n  ..$ mode        : chr \"static\"\n  ..$ edge_default: Named list()\n  ..$ node_default: Named list()\n  ..$ name        : chr \"VAST_MC3_Knowledge_Graph\"\n $ nodes     :'data.frame': 1159 obs. of  31 variables:\n  ..$ type             : chr [1:1159] \"Entity\" \"Entity\" \"Entity\" \"Entity\" ...\n  ..$ label            : chr [1:1159] \"Sam\" \"Kelly\" \"Nadia Conti\" \"Elise\" ...\n  ..$ name             : chr [1:1159] \"Sam\" \"Kelly\" \"Nadia Conti\" \"Elise\" ...\n  ..$ sub_type         : chr [1:1159] \"Person\" \"Person\" \"Person\" \"Person\" ...\n  ..$ id               : chr [1:1159] \"Sam\" \"Kelly\" \"Nadia Conti\" \"Elise\" ...\n  ..$ timestamp        : chr [1:1159] NA NA NA NA ...\n  ..$ monitoring_type  : chr [1:1159] NA NA NA NA ...\n  ..$ findings         : chr [1:1159] NA NA NA NA ...\n  ..$ content          : chr [1:1159] NA NA NA NA ...\n  ..$ assessment_type  : chr [1:1159] NA NA NA NA ...\n  ..$ results          : chr [1:1159] NA NA NA NA ...\n  ..$ movement_type    : chr [1:1159] NA NA NA NA ...\n  ..$ destination      : chr [1:1159] NA NA NA NA ...\n  ..$ enforcement_type : chr [1:1159] NA NA NA NA ...\n  ..$ outcome          : chr [1:1159] NA NA NA NA ...\n  ..$ activity_type    : chr [1:1159] NA NA NA NA ...\n  ..$ participants     : int [1:1159] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ thing_collected  :'data.frame':   1159 obs. of  2 variables:\n  .. ..$ type: chr [1:1159] NA NA NA NA ...\n  .. ..$ name: chr [1:1159] NA NA NA NA ...\n  ..$ reference        : chr [1:1159] NA NA NA NA ...\n  ..$ date             : chr [1:1159] NA NA NA NA ...\n  ..$ time             : chr [1:1159] NA NA NA NA ...\n  ..$ friendship_type  : chr [1:1159] NA NA NA NA ...\n  ..$ permission_type  : chr [1:1159] NA NA NA NA ...\n  ..$ start_date       : chr [1:1159] NA NA NA NA ...\n  ..$ end_date         : chr [1:1159] NA NA NA NA ...\n  ..$ report_type      : chr [1:1159] NA NA NA NA ...\n  ..$ submission_date  : chr [1:1159] NA NA NA NA ...\n  ..$ jurisdiction_type: chr [1:1159] NA NA NA NA ...\n  ..$ authority_level  : chr [1:1159] NA NA NA NA ...\n  ..$ coordination_type: chr [1:1159] NA NA NA NA ...\n  ..$ operational_role : chr [1:1159] NA NA NA NA ...\n $ edges     :'data.frame': 3226 obs. of  5 variables:\n  ..$ id         : chr [1:3226] \"2\" \"3\" \"5\" \"3013\" ...\n  ..$ is_inferred: logi [1:3226] TRUE FALSE TRUE TRUE TRUE TRUE ...\n  ..$ source     : chr [1:3226] \"Sam\" \"Sam\" \"Sam\" \"Sam\" ...\n  ..$ target     : chr [1:3226] \"Relationship_Suspicious_217\" \"Event_Communication_370\" \"Event_Assessment_600\" \"Relationship_Colleagues_430\" ...\n  ..$ type       : chr [1:3226] NA \"sent\" NA NA ...\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNotice that¬†Industry¬†field is in list data type. In general, this data type is not acceptable by¬†tbl_graph()¬†of¬†tidygraph. In order to avoid error arise when building tidygraph object, it is wiser to exclude this field from the edges data table. However, it might be still useful in subsequent analysis.\n\n\n\n\n\nNext,¬†as_tibble()¬†of¬†tibble¬†package package is used to extract the nodes and links tibble data frames from¬†mc3¬†tibble dataframe into two separate tibble dataframes called¬†mc3_nodes¬†and¬†mc3_edges¬†respectively.\n\nmc3_nodes &lt;- as_tibble(MC3$nodes)\nmc3_edges &lt;- as_tibble(MC3$edges)\n\n\n\n\nIt is time for us to apply appropriate EDA methods to examine the data.\nNodes:\n\nThe Code ChunkThe Plots\n\n\n\nExpCatViz(data=mc3_nodes,\n          col=\"lightblue\")\n\n\n\n\nExpCatViz(data=mc3_nodes,\n          col=\"lightblue\")\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\n\n\n\n[[7]]\n\n\n\n\n\n\n\n\n\n\n[[8]]\n\n\n\n\n\n\n\n\n\n\n[[9]]\n\n\n\n\n\n\n\n\n\n\n[[10]]\n\n\n\n\n\n\n\n\n\n\n[[11]]\n\n\n\n\n\n\n\n\n\n\n[[12]]\n\n\n\n\n\n\n\n\n\n\n[[13]]\n\n\n\n\n\n\n\n\n\n\n[[14]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat useful discovery can you obtained from the visualisation above?\n\n\n\n\n\nEdges:\n\nThe Code ChunkThe Plots\n\n\n\nExpCatViz(data=mc3_edges,\n          col=\"lightblue\")\n\n\n\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat useful discovery can you obtained from the visualisation above?"
  },
  {
    "objectID": "TH3/Take-Home_Ex02_MC3.html#data-cleaning-and-wrangling",
    "href": "TH3/Take-Home_Ex02_MC3.html#data-cleaning-and-wrangling",
    "title": "Take-Home Assignment 2",
    "section": "",
    "text": "Code chunk below performs the following data cleaning tasks:\n\nconvert values in id field into character data type,\nexclude records with id value are na,\nexclude records with similar id values,\nexclude thing_collected field, and\nsave the cleaned tibble dataframe into a new tibble datatable called mc3_nodes_cleaned.\n\n\n\nCode\nmc3_nodes_cleaned &lt;- mc3_nodes %&gt;%\n  mutate(id = as.character(id)) %&gt;%\n  filter(!is.na(id)) %&gt;%\n  distinct(id, .keep_all = TRUE) %&gt;%\n  select(-thing_collected)\n\n\n\n\n\nNext, the code chunk below will be used to:\n\nrename source and target fields to from_id and to_id respectively,\nconvert values in from_id and to_id fields to character data type,\nexclude values in from_id and to_id which not found in the id field of mc3_nodes_cleaned,\nexclude records whereby from_id and/or to_id values are missing, and\nsave the cleaned tibble dataframe and called it mc3_edges_cleaned.\n\n\n\nCode\nmc3_edges_cleaned &lt;- mc3_edges %&gt;%\n  rename(from_id = source, \n         to_id = target) %&gt;%\n  mutate(across(c(from_id, to_id), \n                as.character)) %&gt;%\n  filter(from_id %in% mc3_nodes_cleaned$id, \n         to_id %in% mc3_nodes_cleaned$id) %&gt;%\n  filter(!is.na(from_id), !is.na(to_id))\n\n\nNext, code chunk below will be used to create mapping of character id in¬†mc3_nodes_cleaned¬†to row index.\n\n\nCode\nnode_index_lookup &lt;- mc3_nodes_cleaned %&gt;%\n  mutate(.row_id = row_number()) %&gt;%\n  select(id, .row_id)\n\n\nNext, the code chunk below will be used to join and convert¬†from_id¬†and¬†to_id¬†to integer indices. At the same time we also drop rows with unmatched nodes.\n\n\nCode\nmc3_edges_indexed &lt;- mc3_edges_cleaned %&gt;%\n  left_join(node_index_lookup, \n            by = c(\"from_id\" = \"id\")) %&gt;%\n  rename(from = .row_id) %&gt;%\n  left_join(node_index_lookup, \n            by = c(\"to_id\" = \"id\")) %&gt;%\n  rename(to = .row_id) %&gt;%\n  select(from, to, is_inferred, type) %&gt;%\n  filter(!is.na(from) & !is.na(to))  \n\n\nNext the code chunk below is used to subset nodes to only those referenced by edges.\n\n\nCode\nused_node_indices &lt;- sort(\n  unique(c(mc3_edges_indexed$from, \n           mc3_edges_indexed$to)))\n\nmc3_nodes_final &lt;- mc3_nodes_cleaned %&gt;%\n  slice(used_node_indices) %&gt;%\n  mutate(new_index = row_number())\n\n\nWe will then use the code chunk below to rebuild lookup from old index to new index.\n\n\nCode\nold_to_new_index &lt;- tibble(\n  old_index = used_node_indices,\n  new_index = seq_along(\n    used_node_indices))\n\n\nLastly, the code chunk below will be used to update edge indices to match new node table.\n\n\nCode\nmc3_edges_final &lt;- mc3_edges_indexed %&gt;%\n  left_join(old_to_new_index, \n            by = c(\"from\" = \"old_index\")) %&gt;%\n  rename(from_new = new_index) %&gt;%\n  left_join(old_to_new_index, \n            by = c(\"to\" = \"old_index\")) %&gt;%\n  rename(to_new = new_index) %&gt;%\n  select(from = from_new, to = to_new, \n         is_inferred, type)\n\n\n\n\n\nNow we are ready to build the tidygraph object by using the code chunk below.\n\nmc3_graph &lt;- tbl_graph(\n  nodes = mc3_nodes_final,\n  edges = mc3_edges_final,\n  directed = TRUE\n)\n\nAfter the tidygraph object is created, it is always a good practice to examine the object by using¬†str().\n\nstr(mc3_graph)\n\nClasses 'tbl_graph', 'igraph'  hidden list of 10\n $ : num 1159\n $ : logi TRUE\n $ : num [1:3226] 0 0 0 0 0 0 0 1 1 1 ...\n $ : num [1:3226] 1137 356 746 894 875 ...\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ :List of 4\n  ..$ : num [1:3] 1 0 1\n  ..$ : Named list()\n  ..$ :List of 31\n  .. ..$ type             : chr [1:1159] \"Entity\" \"Entity\" \"Entity\" \"Entity\" ...\n  .. ..$ label            : chr [1:1159] \"Sam\" \"Kelly\" \"Nadia Conti\" \"Elise\" ...\n  .. ..$ name             : chr [1:1159] \"Sam\" \"Kelly\" \"Nadia Conti\" \"Elise\" ...\n  .. ..$ sub_type         : chr [1:1159] \"Person\" \"Person\" \"Person\" \"Person\" ...\n  .. ..$ id               : chr [1:1159] \"Sam\" \"Kelly\" \"Nadia Conti\" \"Elise\" ...\n  .. ..$ timestamp        : chr [1:1159] NA NA NA NA ...\n  .. ..$ monitoring_type  : chr [1:1159] NA NA NA NA ...\n  .. ..$ findings         : chr [1:1159] NA NA NA NA ...\n  .. ..$ content          : chr [1:1159] NA NA NA NA ...\n  .. ..$ assessment_type  : chr [1:1159] NA NA NA NA ...\n  .. ..$ results          : chr [1:1159] NA NA NA NA ...\n  .. ..$ movement_type    : chr [1:1159] NA NA NA NA ...\n  .. ..$ destination      : chr [1:1159] NA NA NA NA ...\n  .. ..$ enforcement_type : chr [1:1159] NA NA NA NA ...\n  .. ..$ outcome          : chr [1:1159] NA NA NA NA ...\n  .. ..$ activity_type    : chr [1:1159] NA NA NA NA ...\n  .. ..$ participants     : int [1:1159] NA NA NA NA NA NA NA NA NA NA ...\n  .. ..$ reference        : chr [1:1159] NA NA NA NA ...\n  .. ..$ date             : chr [1:1159] NA NA NA NA ...\n  .. ..$ time             : chr [1:1159] NA NA NA NA ...\n  .. ..$ friendship_type  : chr [1:1159] NA NA NA NA ...\n  .. ..$ permission_type  : chr [1:1159] NA NA NA NA ...\n  .. ..$ start_date       : chr [1:1159] NA NA NA NA ...\n  .. ..$ end_date         : chr [1:1159] NA NA NA NA ...\n  .. ..$ report_type      : chr [1:1159] NA NA NA NA ...\n  .. ..$ submission_date  : chr [1:1159] NA NA NA NA ...\n  .. ..$ jurisdiction_type: chr [1:1159] NA NA NA NA ...\n  .. ..$ authority_level  : chr [1:1159] NA NA NA NA ...\n  .. ..$ coordination_type: chr [1:1159] NA NA NA NA ...\n  .. ..$ operational_role : chr [1:1159] NA NA NA NA ...\n  .. ..$ new_index        : int [1:1159] 1 2 3 4 5 6 7 8 9 10 ...\n  ..$ :List of 2\n  .. ..$ is_inferred: logi [1:3226] TRUE FALSE TRUE TRUE TRUE TRUE ...\n  .. ..$ type       : chr [1:3226] NA \"sent\" NA NA ...\n $ :&lt;environment: 0x0000023a869743f0&gt; \n - attr(*, \"active\")= chr \"nodes\""
  },
  {
    "objectID": "TH3/Take-Home_Ex02_MC3.html#exploratory-data-analysis-after-cleaning-wrangling",
    "href": "TH3/Take-Home_Ex02_MC3.html#exploratory-data-analysis-after-cleaning-wrangling",
    "title": "Take-Home Assignment 2",
    "section": "",
    "text": "Several of the¬†ggraph¬†layouts involve randomisation. In order to ensure reproducibility, it is necessary to set the seed value before plotting by using the code chunk below.\n\nset.seed(1818)\n\n\n\nShows how many nodes are of type Entity, Event, or Relationship.\n\n\nCode\nmc3_nodes_final %&gt;%\n  count(type, sort = TRUE) %&gt;%\n  ggplot(aes(x = reorder(type, -n), y = n, fill = type)) +\n  geom_col() +\n  geom_text(aes(label = n), vjust = -0.3) +\n  labs(title = \"Node Type Distribution\", x = \"Type\", y = \"Count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nIn the code chunk below,¬†ggraph¬†functions are used to create the whole graph.\n\n\nCode\nggraph(mc3_graph, \n       layout = \"fr\") +\n  geom_edge_link(alpha = 0.3, \n                 colour = \"gray\") +\n  geom_node_point(aes(color = `type`), \n                  size = 4) +\n  geom_node_text(aes(label = type), \n                 repel = TRUE, \n                 size = 2.5) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\nFocuses on what kinds of actors are in the graph ‚Äî Person, Vessel, Organization, etc.\n\n\nCode\n# Define color mapping\nsubtype_colors &lt;- c(\n  \"Person\" = \"#2ca5ff\",\n  \"Organization\" = \"#f5ee15\",\n  \"Vessel\" = \"#FB7E81\",\n  \"Group\" = \"#25e158\",\n  \"Location\" = \"#ec4bff\"\n)\n\nmc3_nodes_final %&gt;%\n  filter(type == \"Entity\") %&gt;%\n  count(sub_type, sort = TRUE) %&gt;%\n  ggplot(aes(x = reorder(sub_type, n), y = n, fill = sub_type)) +\n  geom_col() +\n  coord_flip() +\n  geom_text(aes(label = n), hjust = -0.1) +\n  labs(title = \"Entity Sub-type Distribution\", x = \"Sub-type\", y = \"Count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\nTo understand what kinds of actions dominate ‚Äî Communication, Monitoring, Assessment, etc.\n\n\nCode\nmc3_nodes_final %&gt;%\n  filter(type == \"Event\") %&gt;%\n  count(sub_type, sort = TRUE) %&gt;%\n  ggplot(aes(x = reorder(sub_type, n), y = n, fill = sub_type)) +\n  geom_col() +\n  coord_flip() +\n  geom_text(aes(label = n), hjust = -0.1) +\n  labs(title = \"Event Sub-type Distribution\", x = \"Sub-type\", y = \"Count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\nThis finds all Entities that sent or received communication events ‚Äî i.e., actors who participated in messaging.\n\n\nCode\nlibrary(DT)\n\n# Step 1: Get all Communication Event IDs\ncomm_event_ids &lt;- mc3_nodes_cleaned %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  pull(id)\n\n# Step 2: Extract 'sent' edges for communication events\ncomm_sent_edges &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"sent\", to_id %in% comm_event_ids) %&gt;%\n  select(comm_id = to_id, sender_id = from_id)\n\n# Step 3: Extract 'received' edges for same communication events\ncomm_received_edges &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"received\", from_id %in% comm_event_ids) %&gt;%\n  select(comm_id = from_id, receiver_id = to_id)\n\n# Step 4: Join sent and received edges by communication ID\ncomm_pairs &lt;- comm_sent_edges %&gt;%\n  inner_join(comm_received_edges, by = \"comm_id\")\n\n# Step 5: Add sender and receiver labels\nparticipants_named &lt;- comm_pairs %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, sender_label = label), by = c(\"sender_id\" = \"id\")) %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, receiver_label = label), by = c(\"receiver_id\" = \"id\"))\n\n\n\n# Step7: Interactive summary of top sender‚Äìreceiver pairs\nparticipants_named %&gt;%\n  count(sender_label, receiver_label, sort = TRUE) %&gt;%\n  datatable(\n    caption = \"Top Communication Pairs (Sender ‚Üí Receiver)\",\n    colnames = c(\"Sender\", \"Receiver\", \"Message Count\"),\n    options = list(pageLength = 10, autoWidth = TRUE),\n    rownames = FALSE\n  )\n\n\n\n\n\n\n\n\n\nThis code creates an interactive communication network graph using visNetwork, where:\n\nEach node represents a person or entity, node size is based on total messages sent by that participant.\nEach edge (arrow) represents a communication sent from one participant to another, the thicker the edge, the more message sent to that particular receiver.\n\nVer 1: Layout_in_circle\nMessage Senders are arranged from most to the least number of messages sent.\n\n\nCode\nlibrary(visNetwork)\n\n# Step 1: Summarize communication edges\ncomm_edges_vis &lt;- participants_named %&gt;%\n  count(sender_id, receiver_id, sort = TRUE) %&gt;%\n  rename(from = sender_id, to = receiver_id, value = n)\n\n# Step 2: Compute messages sent per node\nmessage_counts &lt;- comm_edges_vis %&gt;%\n  group_by(from) %&gt;%\n  summarise(sent_count = sum(value), .groups = \"drop\")\n\n# Step 3: Prepare nodes, merge with message count and add color/shape\nnodes_vis &lt;- mc3_nodes_cleaned %&gt;%\n  filter(id %in% unique(c(comm_edges_vis$from, comm_edges_vis$to))) %&gt;%\n  select(id, label, sub_type) %&gt;%\n  left_join(message_counts, by = c(\"id\" = \"from\")) %&gt;%\n  mutate(\n    sent_count = replace_na(sent_count, 0),\n    size = rescale(sent_count, to = c(10, 40)),\n    title = paste0(label, \"&lt;br&gt;Sub-type: \", sub_type,\n                   ifelse(!is.na(sent_count), paste0(\"&lt;br&gt;Sent: \", sent_count, \" messages\"), \"\")),\n    color = case_when(\n      sub_type == \"Person\" ~ \"#2ca5ff\",\n      sub_type == \"Organization\" ~ \"#f5ee15\",\n      sub_type == \"Vessel\" ~ \"#FB7E81\",\n      sub_type == \"Group\" ~ \"#25e158\",\n      sub_type == \"Location\" ~ \"#ec4bff\",\n      TRUE ~ \"black\"\n    ),\n    shape = case_when(\n      sub_type == \"Person\" ~ \"dot\",\n      sub_type == \"Organization\" ~ \"square\",\n      sub_type == \"Vessel\" ~ \"triangle\",\n      sub_type == \"Group\" ~ \"star\",\n      sub_type == \"Location\" ~ \"diamond\",\n      TRUE ~ \"dot\"\n    ),\n  ) %&gt;%\n  arrange(desc(size))\n\n# Step 4: Format visNetwork edges\nedges_vis &lt;- comm_edges_vis %&gt;%\n  mutate(\n    arrows = \"to\",\n    width = rescale(value, to = c(1, 6)),\n    title = paste(\"Messages:\", value)\n  )\n\n# Step 5: Define legend items\nlegend_nodes &lt;- data.frame(\n  label = c(\"Person\", \"Organization\", \"Vessel\", \"Group\", \"Location\"),\n  color = c(\"#2ca5ff\", \"#f5ee15\", \"#FB7E81\", \"#25e158\", \"#ec4bff\"),\n  shape = c(\"dot\", \"square\", \"triangle\", \"star\", \"diamond\"),\n  stringsAsFactors = FALSE\n)\n\n# Step 6: Render network with legend\nvisNetwork(nodes_vis, edges_vis, width = \"100%\", height = \"1000px\") %&gt;%\n  visNodes(\n    size = nodes_vis$size\n    # color and shape are picked up from nodes_vis columns automatically\n  ) %&gt;%\n  visLegend(\n    addNodes = lapply(1:nrow(legend_nodes), function(i) {\n      list(\n        label = legend_nodes$label[i],\n        shape = legend_nodes$shape[i],\n        color = legend_nodes$color[i]\n      )\n    }),\n    useGroups = FALSE,\n    width = 0.15\n  ) %&gt;%\n  visEdges(smooth = FALSE) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visIgraphLayout(layout = \"layout_in_circle\") %&gt;%\n  visPhysics(enabled = FALSE) %&gt;%\n  visLayout(randomSeed = 1818)\n\n\n\n\n\n\nVer 2: Layout_on_sphere\nFrom this plot, it reveals that some pairs (e.g., Miranda Jordan and Clepper Jensen) mainly communicate with each other, suggesting isolated or private channels outside the broader network.\n\n\nCode\nlibrary(visNetwork)\n\n# Step 1: Summarize communication edges\ncomm_edges_vis &lt;- participants_named %&gt;%\n  count(sender_id, receiver_id, sort = TRUE) %&gt;%\n  rename(from = sender_id, to = receiver_id, value = n)\n\n# Step 2: Compute messages sent per person (by sender)\nmessage_counts &lt;- comm_edges_vis %&gt;%\n  group_by(from) %&gt;%\n  summarise(sent_count = sum(value), .groups = \"drop\")\n\n# Step 3: Prepare nodes with label, subtype, color, shape, and scaled size\nnodes_vis &lt;- mc3_nodes_cleaned %&gt;%\n  filter(id %in% unique(c(comm_edges_vis$from, comm_edges_vis$to))) %&gt;%\n  select(id, label, sub_type) %&gt;%\n  left_join(message_counts, by = c(\"id\" = \"from\")) %&gt;%\n  mutate(\n    size = if_else(\n      sub_type == \"Person\",\n      rescale(sent_count, to = c(10, 40), na.rm = TRUE),\n      15\n    ),\n    title = paste0(label, \"&lt;br&gt;Sub-type: \", sub_type,\n                   ifelse(!is.na(sent_count), paste0(\"&lt;br&gt;Sent: \", sent_count, \" messages\"), \"\")),\n    color = case_when(\n      sub_type == \"Person\" ~ \"#2ca5ff\",\n      sub_type == \"Organization\" ~ \"#f5ee15\",\n      sub_type == \"Vessel\" ~ \"#FB7E81\",\n      sub_type == \"Group\" ~ \"#25e158\",\n      sub_type == \"Location\" ~ \"#ec4bff\",\n      TRUE ~ \"black\"\n    ),\n    shape = case_when(\n      sub_type == \"Person\" ~ \"dot\",\n      sub_type == \"Organization\" ~ \"square\",\n      sub_type == \"Vessel\" ~ \"triangle\",\n      sub_type == \"Group\" ~ \"star\",\n      sub_type == \"Location\" ~ \"diamond\",\n      TRUE ~ \"dot\"\n    )\n  )\n\n# Step 4: Format edges\nedges_vis &lt;- comm_edges_vis %&gt;%\n  mutate(\n    arrows = \"to\",\n    width = rescale(value, to = c(1, 6)),\n    title = paste(\"Messages:\", value)\n  )\n\n# Step 5: Legend mapping\nlegend_nodes &lt;- data.frame(\n  label = c(\"Person\", \"Organization\", \"Vessel\", \"Group\", \"Location\"),\n  color = c(\"#2ca5ff\", \"#f5ee15\", \"#FB7E81\", \"#25e158\", \"#ec4bff\"),\n  shape = c(\"dot\", \"square\", \"triangle\", \"star\", \"diamond\"),\n  stringsAsFactors = FALSE\n)\n\n# Step 6: Render the network with layout_on_sphere and legend\nvisNetwork(nodes_vis, edges_vis, width = \"100%\", height = \"900px\") %&gt;%\n  visNodes(\n    size = nodes_vis$size\n    # color and shape columns are automatically used\n  ) %&gt;%\n  visLegend(\n    addNodes = lapply(1:nrow(legend_nodes), function(i) {\n      list(\n        label = legend_nodes$label[i],\n        shape = legend_nodes$shape[i],\n        color = legend_nodes$color[i]\n      )\n    }),\n    useGroups = FALSE,\n    width = 0.15\n  ) %&gt;%\n  visEdges(smooth = FALSE) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visIgraphLayout(layout = \"layout_on_sphere\") %&gt;%\n  visPhysics(enabled = FALSE) %&gt;%\n  visLayout(randomSeed = 1818)"
  },
  {
    "objectID": "TH3/Take-Home_Ex02_MC3.html#task-1a-1b-daily-temporal-patterns-in-communications-over-the-two-weeks",
    "href": "TH3/Take-Home_Ex02_MC3.html#task-1a-1b-daily-temporal-patterns-in-communications-over-the-two-weeks",
    "title": "Take-Home Assignment 2",
    "section": "",
    "text": "VAST Challenge Task & Question 1a and 1b\n\n\n\nClepper found that messages frequently came in at around the same time each day.\n\nDevelop a graph-based visual analytics approach to identify any daily temporal patterns in communications.\nHow do these patterns shift over the two weeks of observations?\n\n\n\nObjective\n\nIdentify when communications happen most often during each day.\nDetect shifts in these patterns over the 2-week period.\nLater: Focus on a specific entity (e.g., Nadia Conti) and explore who influences them.\n\n\n\nExtract the Communication Timestamps from mc3_nodes_final and filter for communication events.\n\n# Filter for Communication events\ncomm_events &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(timestamp = ymd_hms(timestamp)) %&gt;%\n  filter(!is.na(timestamp)) %&gt;%\n  mutate(\n    day = as.Date(timestamp),\n    hour = hour(timestamp)\n  )\n\nParse the Communication Timestamp into the format ‚Äúdd/mm/yyy (ddd)‚Äù for ease of reference.\n\n# Communication events with parsed date and time\ncomm_events &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(timestamp = ymd_hms(timestamp)) %&gt;%\n  filter(!is.na(timestamp)) %&gt;%\n  mutate(\n    hour = hour(timestamp),\n    date_label = format(timestamp, \"%d/%m/%Y (%a)\")  # e.g., \"19/03/2040 (Tue)\"\n  )\n\n\n\n\n\n\n\n\nCode\n# Step 1: Prepare daily message volume data\ndaily_message_volume &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(\n    timestamp = ymd_hms(timestamp),\n    date = as.Date(timestamp),\n    date_label = format(timestamp, \"%d/%m/%Y (%a)\")\n  ) %&gt;%\n  group_by(date, date_label) %&gt;%\n  summarise(message_count = n(), .groups = \"drop\") %&gt;%\n  arrange(date)\n\n# Step 2: Compute average and total message count\navg_msg_count &lt;- mean(daily_message_volume$message_count)\ntotal_msg_count &lt;- sum(daily_message_volume$message_count)\n\n# Step 3: Plot bar chart with average + total labels\nggplot(daily_message_volume, aes(x = date_label, y = message_count)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  geom_text(\n    aes(label = message_count),\n    vjust = -0.3,\n    size = 2.5,\n    color = \"grey40\"\n  ) +\n  geom_hline(yintercept = avg_msg_count, color = \"red\", linetype = \"dashed\", size = 1.2) +\n  annotate(\n    \"label\", x = 1, y = avg_msg_count + 2,\n    label = paste(\"Average =\", round(avg_msg_count, 1)),\n    color = \"red\", fill = \"grey90\",\n    label.size = 0, hjust = -0.2, vjust = 3\n  ) +\n  annotate(\n    \"label\", x = nrow(daily_message_volume), y = max(daily_message_volume$message_count) + 5,\n    label = paste(\"Total =\", total_msg_count),\n    color = \"black\", fill = \"lightgrey\",\n    label.size = 0.3, hjust = 1.1, vjust = 1\n  ) +\n  labs(\n    title = \"Daily Radio Communication Volume\",\n    x = \"Date\",\n    y = \"Message Count\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(DT)\n\n# Daily message volume with comparisons\ndaily_message_volume &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(\n    timestamp = ymd_hms(timestamp),\n    date = as.Date(timestamp),\n    date_label = format(timestamp, \"%d/%m/%Y (%a)\")\n  ) %&gt;%\n  group_by(date, date_label) %&gt;%\n  summarise(message_count = n(), .groups = \"drop\") %&gt;%\n  arrange(date) %&gt;%\n  mutate(\n    change_from_prev = message_count - lag(message_count),\n    pct_change_from_prev = round((message_count - lag(message_count)) / lag(message_count) * 100, 2)\n  )\n\ndatatable(\n  daily_message_volume %&gt;% select(-date),  # remove raw date if not needed\n  caption = \"Daily Message Volume with Day-over-Day Change\",\n  options = list(pageLength = 14, order = list(list(0, 'asc'))),\n  rownames = FALSE\n)\n\n\n\n\n\n\n\n\n\nThis heat map is interactive and you may choose to hover on the tile to display the date, time, and message count\n\n\nCode\nlibrary(forcats)\nlibrary(plotly)\n\n# Step 1: Reconstruct sender‚Äìreceiver‚Äìtimestamp structure\ncomm_events_raw &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  select(event_id = id, timestamp) %&gt;%\n  mutate(timestamp = ymd_hms(timestamp),\n         hour = hour(timestamp),\n         date_label = format(timestamp, \"%d/%m/%Y (%a)\"))\n\n# Step 2: Get sender (sent) and receiver (received) links\ncomm_edges_sent &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"sent\") %&gt;%\n  select(event_id = to_id, sender_id = from_id)\n\ncomm_edges_recv &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"received\") %&gt;%\n  select(event_id = from_id, receiver_id = to_id)\n\n# Step 3: Join all together into sender‚Äìreceiver‚Äìtimestamp\ncomm_links &lt;- comm_events_raw %&gt;%\n  left_join(comm_edges_sent, by = \"event_id\") %&gt;%\n  left_join(comm_edges_recv, by = \"event_id\") %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(sender_id = id, sender_label = label), by = \"sender_id\") %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(receiver_id = id, receiver_label = label), by = \"receiver_id\")\n\n# Step 4: Aggregate total messages per hour/day\ncomm_heatmap &lt;- comm_links %&gt;%\n  group_by(date_label, hour) %&gt;%\n  summarise(\n    count = n(),\n    top_sender = names(sort(table(sender_label), decreasing = TRUE))[1],\n    sender_count = max(table(sender_label)),\n    top_receiver = names(sort(table(receiver_label), decreasing = TRUE))[1],\n    receiver_count = max(table(receiver_label)),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    tooltip = paste0(\n      \"üìÖ Date: \", date_label,\n      \"&lt;br&gt;‚è∞ Hour: \", sprintf(\"%02d:00\", hour),\n      \"&lt;br&gt;üì® Messages: \", count,\n      \"&lt;br&gt;üî¥ Top Sender: \", top_sender, \" (\", sender_count, \")\",\n      \"&lt;br&gt;üü¢ Top Receiver: \", top_receiver, \" (\", receiver_count, \")\"\n    )\n  )\n\n# Step 5: Static ggplot\np &lt;- ggplot(comm_heatmap, aes(\n  x = hour,\n  y = fct_rev(factor(date_label)),\n  fill = count,\n  text = tooltip\n)) +\n  geom_tile(color = \"white\") +\n  scale_fill_viridis_c(option = \"inferno\", direction = -1, name = \"Message Count\") +\n  scale_x_continuous(\n    breaks = 0:23,\n    labels = function(x) sprintf(\"%02d:00\", x)\n  ) +\n  labs(\n    title = \"Hourly Heatmap of Radio Communications by Day\",\n    x = \"Hour of Day\",\n    y = NULL\n  ) +\n  theme_minimal(base_size = 10) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid = element_blank()\n  )\n\n# Step 6: Make interactive\nggplotly(p, tooltip = \"text\")\n\n\n\n\n\n\nWe will increase the resolution to half-hour time slots.\n\n\n\nThis heat map is interactive and you may choose to hover on the tile to display the date, time, and message count.\n\n\nCode\nlibrary(forcats)\nlibrary(plotly)\n\n# Step 1: Fix sender and receiver edges\ncomm_edges_sent &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"sent\") %&gt;%\n  select(event_id = to_id, sender_id = from_id)\n\ncomm_edges_recv &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"received\") %&gt;%\n  select(event_id = from_id, receiver_id = to_id)  # ‚úÖ fixed receiver_id\n\n# Step 2: Reconstruct sender‚Äìreceiver‚Äìevent linkage\ncomm_events_raw &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  select(event_id = id, timestamp) %&gt;%\n  mutate(\n    timestamp = ymd_hms(timestamp),\n    hour = hour(timestamp),\n    minute = minute(timestamp),\n    time_bin = hour + ifelse(minute &lt; 30, 0, 0.5),\n    date_label = format(timestamp, \"%d/%m/%Y (%a)\"),\n    time_label = sprintf(\"%02d:%02d\", floor(time_bin), ifelse(time_bin %% 1 == 0, 0, 30))\n  )\n\n# Step 3: Join to get sender/receiver labels\ncomm_links &lt;- comm_events_raw %&gt;%\n  left_join(comm_edges_sent, by = \"event_id\") %&gt;%\n  left_join(comm_edges_recv, by = \"event_id\") %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, sender_label = label), by = c(\"sender_id\" = \"id\")) %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, receiver_label = label), by = c(\"receiver_id\" = \"id\"))\n\n# Step 4: Aggregate by half-hour + label top actors\ncomm_heatmap &lt;- comm_links %&gt;%\n  group_by(date_label, time_bin, time_label) %&gt;%\n  summarise(\n    count = n(),\n    top_sender = names(sort(table(sender_label), decreasing = TRUE))[1],\n    sender_count = max(table(sender_label)),\n    top_receiver = names(sort(table(receiver_label), decreasing = TRUE))[1],\n    receiver_count = max(table(receiver_label)),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    tooltip = paste0(\n      \"üìÖ Date: \", date_label,\n      \"&lt;br&gt;üïí Time: \", time_label,\n      \"&lt;br&gt;üì® Messages: \", count,\n      \"&lt;br&gt;üî¥ Top Sender: \", top_sender, \" (\", sender_count, \")\",\n      \"&lt;br&gt;üü¢ Top Receiver: \", top_receiver, \" (\", receiver_count, \")\"\n    )\n  )\n\n# Step 5: ggplot\np &lt;- ggplot(comm_heatmap, aes(x = time_bin, y = fct_rev(factor(date_label)), fill = count, text = tooltip)) +\n  geom_tile(color = \"white\") +\n  scale_fill_viridis_c(\n    option = \"inferno\",\n    direction = -1,\n    name = \"Message Count\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  labs(\n    title = \"Half-Hourly Heatmap of Radio Communications by Day\",\n    x = \"Time of Day\",\n    y = NULL\n  ) +\n  theme_minimal(base_size = 10) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid = element_blank()\n  )\n\n# Step 6: Convert to interactive Plotly plot\nggplotly(p, tooltip = \"text\")\n\n\n\n\n\n\n\n\n\nThe faceted density plot that shows the distribution of communication events by time of day, broken down for each day in the dataset. It helps to visually detect temporal communication patterns, intensity, and consistency over multiple days.\n\nOverview of the 2 week periodDay 1 - 01/10/2040Day 2 - 02/10/2040Day 3 - 03/10/2040Day 4 - 04/10/2040Day 5 - 05/10/2040Day 6 - 06/10/2040Day 7 - 07/10/2040Day 8 - 08/10/2040Day 9 - 09/10/2040Day 10 - 10/10/2040Day 11 - 11/10/2040Day 12 - 12/10/2040Day 13 - 13/10/2040Day 14 - 14/10/2040\n\n\n\n\nCode\n# Step 1: Preprocess communication events\ncomm_events &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(timestamp = ymd_hms(timestamp)) %&gt;%\n  filter(!is.na(timestamp)) %&gt;%\n  mutate(\n    date_label = format(timestamp, \"%d/%m/%Y (%a)\"),\n    hour = hour(timestamp),\n    minute = minute(timestamp),\n    time_bin = hour + ifelse(minute &lt; 30, 0, 0.5)\n  )\n\n# Step 2: Summarise daily medians and counts\ndaily_stats &lt;- comm_events %&gt;%\n  group_by(date_label) %&gt;%\n  summarise(\n    median_time = median(time_bin),\n    msg_count = n(),\n    .groups = \"drop\"\n  )\n\n# Step 3: Plot\nggplot(comm_events, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = daily_stats, aes(xintercept = median_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(\n    data = daily_stats,\n    aes(x = 20.5, y = 0.25, label = paste(\"Total:\", msg_count)),\n    inherit.aes = FALSE,\n    size = 3,\n    color = \"grey20\",\n    hjust = 1\n  ) +\n  facet_wrap(~ date_label, ncol = 4) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = NULL  # suppress all x-axis labels\n  ) +\n  labs(\n    title = \"Daily Communication Patterns (Half-Hourly)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"01/10/2040 (Mon)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"02/10/2040 (Tue)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"03/10/2040 (Wed)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"04/10/2040 (Thu)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"05/10/2040 (Fri)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"06/10/2040 (Sat)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"07/10/2040 (Sun)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"08/10/2040 (Mon)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"09/10/2040 (Tue)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"10/10/2040 (Wed)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"11/10/2040 (Thu)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"12/10/2040 (Fri)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"13/10/2040 (Sat)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"14/10/2040 (Sun)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ¬±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nüìà Insights This Visualization Offers\n\n\n\n\nBar Plot of combined hourly message volume over the 2 weeks period:\n\n\nCode\n# Prepare data\ncomm_hourly &lt;- comm_events %&gt;%\n  count(hour) %&gt;%\n  mutate(\n    hour_label = sprintf(\"%02d:00\", hour),  # Format to hh:mm\n    percent = n / sum(n)\n  )\n\n# Plot\nggplot(comm_hourly, aes(x = hour_label, y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  geom_text_repel(\n    aes(label = paste0(n, \" (\", percent(percent, accuracy = 1), \")\")),\n    nudge_y = 3,\n    size = 2.5,\n    direction = \"y\",\n    max.overlaps = Inf\n  ) +\n  labs(\n    title = \"Overall Hourly Communication Volume\",\n    x = \"Time of Day (hh:mm)\",\n    y = \"Message Count\"\n  ) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nBar Plot of combined half-hourly message volume in the 2 weeks period.\n\n\nCode\ncomm_events &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(timestamp = ymd_hms(timestamp)) %&gt;%\n  filter(!is.na(timestamp)) %&gt;%\n  mutate(\n    hour = hour(timestamp),\n    minute = minute(timestamp),\n    time_bin = sprintf(\"%02d:%02d\", hour, ifelse(minute &lt; 30, 0, 30))\n  )\n\ncomm_halfhour &lt;- comm_events %&gt;%\n  count(time_bin) %&gt;%\n  mutate(percent = n / sum(n))\n\nggplot(comm_halfhour, aes(x = time_bin, y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  geom_text_repel(\n    aes(label = paste0(n, \" (\", percent(percent, accuracy = 1), \")\")),\n    nudge_y = 3,\n    size = 2.5,\n    direction = \"y\",\n    max.overlaps = Inf\n  ) +\n  labs(\n    title = \"Overall Half-Hourly Communication Volume\",\n    x = \"Time of Day (hh:mm)\",\n    y = \"Message Count\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1a. What are the identifiable daily temporal patterns in communications?\n\n\n\n\nThe daily communication volume fluctuates slightly between 34 and 49 messages, with an average of approximately 42 messages per day, highlighting a stable overall activity level. Notably, the highest volume occurs on 11th October (49 messages), immediately following the lowest volume the day before on 10th October (34 messages)‚Äîa sharp rebound that may signal a response to specific events or operational needs. Despite these fluctuations, the system maintains a consistent tempo across the two weeks.\nThe temporal analysis using both the heat map and time series plots reveals a pronounced morning-centric communication rhythm. The vast majority of radio traffic is concentrated between 9:00 AM and 11:30 AM, with the most intense peaks typically occurring between 10:00 and 11:00 AM. With reference to the Density plot of Daily half-hourly message volume, of the 14 days, we see message density peaks at 10:30 AM on 9 days, while on 3 days, it peaks at 12:30 PM.\nFor instance if we were to based in on the hourly plot, 5th October (Fri) and 11th October (Thu) both register their highest single-hour counts at 10:00 AM at 24 and 21 messages respectively. Communication activity drops off steeply after lunchtime, with more than 90% of the days showing little to no activity after 2:30 PM. This pattern suggests a highly structured daily workflow, where key decisions and coordination are front-loaded in the day. Importantly, the hourly heat map also indicates that this routine holds across both weekdays and weekends‚Äîcommunication volumes and peak hours remain similar, underlining the operational regularity of the group regardless of the day of week.\n\n\n\n\n\n\n\n\n\n1b. How do these patterns shift over the two weeks of observations?\n\n\n\n\nOver the two-week period, while the timing and structure of communication peaks remain broadly consistent, there are subtle shifts in both intensity and timing. Some days, such as 3rd, 5th, 11th and 12th October, see particularly high spikes in the mid-morning, which may correspond to critical events, decision points, or heightened urgency. The sharp dip on October 8th and 13th, immediately after a period of ‚Äúsurge‚Äù (3rd - 7th and 9th to 12th October), points to possible responses to interruptions, lulls, or triggering incidents. Overall, although the daily messaging routine is remarkably stable, these bursts and brief lulls provide clues to changing circumstances or stress points in the operation‚Äîan analytical signal that warrants closer inspection of event logs or external triggers for those dates.\nAnother notable change in the communication pattern is observed during the weekends. In the first week, weekend communication peaks occurred earlier, typically between 10:00 AM and 11:30 AM, closely mirroring the weekday rhythm. However, in the second week, the weekend peaks shifted noticeably later, with the highest message volumes concentrated around 12:00 PM and 1:00 PM. This shift not only marks a departure from the otherwise stable early-morning communication structure but also suggests an adaptive or reactive operational schedule‚Äîpotentially in response to evolving events, increased coordination needs, or changing priorities as the observation period progressed. The contrast between the two weekends is clear in the heatmap, underscoring the importance of monitoring such shifts as possible indicators of underlying changes in group behavior or external pressures."
  },
  {
    "objectID": "TH3/Take-Home_Ex02_MC3.html#task-1c-focus-on-a-particular-entity---nadia-conti",
    "href": "TH3/Take-Home_Ex02_MC3.html#task-1c-focus-on-a-particular-entity---nadia-conti",
    "title": "Take-Home Assignment 2",
    "section": "",
    "text": "VAST Challenge Task & Question 1c\n\n\n\nClepper found that messages frequently came in at around the same time each day.\n\nFocus on a specific entity and use this information to determine who has influence over them.\n\n\n\n\n\nWe first extracted the relevant communication edges from the dataset, pairing ‚Äúsent‚Äù and ‚Äúreceived‚Äù communication events to form entity-to-entity links. We retained only those edges where both nodes represent real-world entities (Person, Organization, Vessel, Group, or Location), ensuring that our analysis focuses on the meaningful actors in the Oceanus network.\n\n\nCode\n# Extract sent and received communication event edges\nsent_edges &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"sent\") %&gt;%\n  select(source_entity = from_id, event = to_id)\n\nreceived_edges &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"received\") %&gt;%\n  select(event = from_id, target_entity = to_id)\n\n# Pair sent and received to form communication edges\npaired_edges &lt;- sent_edges %&gt;%\n  inner_join(received_edges, by = \"event\") %&gt;%\n  select(from = source_entity, to = target_entity)\n\n# Add unmatched sent and received edges (optional, for completeness)\nsingle_sent_edges &lt;- sent_edges %&gt;%\n  select(from = source_entity, to = event)\nsingle_received_edges &lt;- received_edges %&gt;%\n  select(from = event, to = target_entity)\n\nall_edges &lt;- bind_rows(paired_edges, single_sent_edges, single_received_edges) %&gt;%\n  distinct()\n\n# Identify entity nodes (Person, Organization, Vessel, Group, Location)\nentity_ids &lt;- mc3_nodes_cleaned %&gt;%\n  filter(sub_type %in% c(\"Person\", \"Organization\", \"Vessel\", \"Group\", \"Location\")) %&gt;%\n  pull(id) %&gt;% as.character()\n\nentity_edges &lt;- all_edges %&gt;%\n  filter(as.character(from) %in% entity_ids, as.character(to) %in% entity_ids)\n\nentity_nodes &lt;- mc3_nodes_cleaned %&gt;%\n  filter(sub_type %in% c(\"Person\", \"Organization\", \"Vessel\", \"Group\", \"Location\")) %&gt;%\n  select(id, label, sub_type)\n\n\n\n\n\nUsing these cleaned and filtered edges and nodes, we built a global directed graph representing the Oceanus community. We then computed key network centrality metrics for each node‚ÄîPageRank, betweenness, and degree‚Äîquantifying the influence and connectivity of every entity in the overall network.\n\n\nCode\nlibrary(igraph)\n\ng &lt;- graph_from_data_frame(d = entity_edges, vertices = entity_nodes, directed = TRUE)\n\n# Compute centralities\nV(g)$pagerank &lt;- page_rank(g)$vector\nV(g)$betweenness &lt;- betweenness(g)\nV(g)$degree &lt;- degree(g)\n\n\n\n\n\nFocusing on ‚ÄúNadia Conti‚Äù, we identified her node and extracted her two-step ego network, capturing both direct and indirect connections within the broader network. This local subgraph reveals Nadia‚Äôs immediate sphere of influence and the key players connected to her.\n\n\nCode\nnadia_label &lt;- \"Nadia Conti\"\ntarget_index &lt;- which(V(g)$label == nadia_label)\n\nego_graph &lt;- make_ego_graph(g, order = 2, nodes = target_index, mode = \"all\")[[1]]\n\n\n\n\n\nWe visualized Nadia‚Äôs ego network using node size, shape, and color to represent centrality and entity type. We also summarized centrality metrics in clear tables, ranking all ego network members by PageRank, Betweenness, and Degree. This allows for direct identification of the most influential, best-connected, and most strategic actors in Nadia Conti‚Äôs communication environment.\n\n\nCode\nnodes_df &lt;- data.frame(\n  id = V(ego_graph)$name,\n  label = V(ego_graph)$label,\n  group = V(ego_graph)$sub_type,\n  title = paste0(\"&lt;b&gt;\", V(ego_graph)$label, \"&lt;/b&gt;&lt;br&gt;\",\n                 \"Degree: \", round(V(ego_graph)$degree, 2), \"&lt;br&gt;\",\n                 \"Betweenness: \", round(V(ego_graph)$betweenness, 2), \"&lt;br&gt;\",\n                 \"PageRank: \", round(V(ego_graph)$pagerank, 4)),\n  shape = ifelse(V(ego_graph)$sub_type == \"Person\", \"dot\",\n                 ifelse(V(ego_graph)$sub_type == \"Organization\", \"square\",\n                        ifelse(V(ego_graph)$sub_type == \"Vessel\", \"triangle\",\n                               ifelse(V(ego_graph)$sub_type == \"Group\", \"star\", \"diamond\")))),\n  value = V(ego_graph)$pagerank * 30 + 5\n)\n\nedges_df &lt;- as_data_frame(ego_graph, what = \"edges\") %&gt;%\n  rename(from = from, to = to)\n\nlibrary(visNetwork)\nvisNetwork(nodes_df, edges_df, width = \"100%\", height = \"700px\") %&gt;%\n  visNodes(scaling = list(min = 5, max = 30)) %&gt;%\n  visEdges(\n    arrows = list(to = list(enabled = TRUE, scaleFactor = 0.3)),\n    color = list(color = \"gray\")\n  ) %&gt;%\n  visOptions(\n    highlightNearest = TRUE,\n    nodesIdSelection = TRUE,\n    manipulation = FALSE\n  ) %&gt;%\n  visInteraction(\n    dragNodes = FALSE,\n    dragView = FALSE,\n    zoomView = FALSE\n  ) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 1818)\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal and Ego-Network Structure\n\n\n\nThe overview network visualization reveals that Nadia Conti is centrally embedded in the Oceanus communication web, maintaining direct and indirect connections with major actors such as Neptune (Vessel), V. Miesel Shipping (Organization), Elise (Person), and others. The use of color and shape coding in the network allows for quick identification of the different types of entities in Nadia‚Äôs influence neighborhood.\n\n\n\n\n\nOn both the global and Nadia-focused ego networks, we computed standard network centrality metrics for all nodes:\n\nPageRank (overall influence),\nBetweenness (information brokerage/intermediary role), and\nDegree (number of direct connections).\n\nThese measures quantify the importance and structural roles of each entity relative to Nadia and the broader community.\n\n\nCode\n# PageRank table\npagerank_df &lt;- data.frame(\n  label = V(ego_graph)$label,\n  sub_type = V(ego_graph)$sub_type,\n  pagerank = round(V(ego_graph)$pagerank, 4)\n) %&gt;% arrange(desc(pagerank))\n\n# Betweenness table\nbetweenness_df &lt;- data.frame(\n  label = V(ego_graph)$label,\n  sub_type = V(ego_graph)$sub_type,\n  betweenness = round(V(ego_graph)$betweenness, 2)\n) %&gt;% arrange(desc(betweenness))\n\n# Degree table\ndegree_df &lt;- data.frame(\n  label = V(ego_graph)$label,\n  sub_type = V(ego_graph)$sub_type,\n  degree = V(ego_graph)$degree\n) %&gt;% arrange(desc(degree))\n\n\n\nknitr::kable(pagerank_df, caption = \"PageRank Centrality (Nadia's Ego Network)\")\n\n\nPageRank Centrality (Nadia‚Äôs Ego Network)\n\n\nlabel\nsub_type\npagerank\n\n\n\n\nMako\nVessel\n0.0687\n\n\nOceanus City Council\nOrganization\n0.0530\n\n\nReef Guardian\nVessel\n0.0454\n\n\nNadia Conti\nPerson\n0.0432\n\n\nRemora\nVessel\n0.0409\n\n\nV. Miesel Shipping\nOrganization\n0.0394\n\n\nNeptune\nVessel\n0.0358\n\n\nHimark Harbor\nLocation\n0.0358\n\n\nLiam Thorne\nPerson\n0.0275\n\n\nBoss\nPerson\n0.0272\n\n\nSentinel\nVessel\n0.0250\n\n\nPaackland Harbor\nLocation\n0.0244\n\n\nDavis\nPerson\n0.0239\n\n\nMarlin\nVessel\n0.0235\n\n\nEcoVigil\nVessel\n0.0233\n\n\nGreen Guardians\nOrganization\n0.0224\n\n\nMrs.¬†Money\nPerson\n0.0192\n\n\nSailor Shifts Team\nOrganization\n0.0186\n\n\nSeawatch\nVessel\n0.0186\n\n\nElise\nPerson\n0.0182\n\n\nSerenity\nVessel\n0.0170\n\n\nHorizon\nVessel\n0.0152\n\n\nThe Middleman\nPerson\n0.0142\n\n\nNorthern Light\nVessel\n0.0135\n\n\nRodriguez\nPerson\n0.0122\n\n\nSamantha Blake\nPerson\n0.0114\n\n\nHaacklee Harbor\nLocation\n0.0111\n\n\nOsprey\nVessel\n0.0088\n\n\nCity Officials\nGroup\n0.0066\n\n\nThe Lookout\nPerson\n0.0062\n\n\nKnowles\nVessel\n0.0051\n\n\nSmall Fry\nPerson\n0.0035\n\n\nGlitters Team\nOrganization\n0.0035\n\n\n\n\n\n\nknitr::kable(betweenness_df, caption = \"Betweenness Centrality (Nadia's Ego Network)\")\n\n\nBetweenness Centrality (Nadia‚Äôs Ego Network)\n\n\nlabel\nsub_type\nbetweenness\n\n\n\n\nMako\nVessel\n368.50\n\n\nMrs.¬†Money\nPerson\n167.18\n\n\nReef Guardian\nVessel\n139.69\n\n\nBoss\nPerson\n136.18\n\n\nV. Miesel Shipping\nOrganization\n118.70\n\n\nNadia Conti\nPerson\n117.87\n\n\nOceanus City Council\nOrganization\n116.11\n\n\nRemora\nVessel\n90.45\n\n\nNeptune\nVessel\n82.59\n\n\nThe Lookout\nPerson\n80.51\n\n\nHimark Harbor\nLocation\n52.61\n\n\nThe Middleman\nPerson\n50.78\n\n\nLiam Thorne\nPerson\n41.81\n\n\nHaacklee Harbor\nLocation\n41.30\n\n\nSentinel\nVessel\n34.54\n\n\nGreen Guardians\nOrganization\n27.51\n\n\nPaackland Harbor\nLocation\n27.08\n\n\nDavis\nPerson\n22.36\n\n\nEcoVigil\nVessel\n12.63\n\n\nRodriguez\nPerson\n11.75\n\n\nNorthern Light\nVessel\n9.76\n\n\nSailor Shifts Team\nOrganization\n7.34\n\n\nHorizon\nVessel\n6.72\n\n\nMarlin\nVessel\n6.23\n\n\nSeawatch\nVessel\n5.20\n\n\nElise\nPerson\n4.60\n\n\nSamantha Blake\nPerson\n4.49\n\n\nSerenity\nVessel\n0.81\n\n\nKnowles\nVessel\n0.50\n\n\nSmall Fry\nPerson\n0.00\n\n\nGlitters Team\nOrganization\n0.00\n\n\nOsprey\nVessel\n0.00\n\n\nCity Officials\nGroup\n0.00\n\n\n\n\n\n\nknitr::kable(degree_df, caption = \"Degree Centrality (Nadia's Ego Network)\")\n\n\nDegree Centrality (Nadia‚Äôs Ego Network)\n\n\nlabel\nsub_type\ndegree\n\n\n\n\nMako\nVessel\n37\n\n\nOceanus City Council\nOrganization\n28\n\n\nReef Guardian\nVessel\n27\n\n\nRemora\nVessel\n21\n\n\nV. Miesel Shipping\nOrganization\n19\n\n\nNeptune\nVessel\n19\n\n\nNadia Conti\nPerson\n17\n\n\nGreen Guardians\nOrganization\n17\n\n\nHimark Harbor\nLocation\n17\n\n\nDavis\nPerson\n16\n\n\nSentinel\nVessel\n16\n\n\nBoss\nPerson\n13\n\n\nEcoVigil\nVessel\n13\n\n\nPaackland Harbor\nLocation\n13\n\n\nMrs.¬†Money\nPerson\n12\n\n\nHorizon\nVessel\n12\n\n\nLiam Thorne\nPerson\n11\n\n\nRodriguez\nPerson\n10\n\n\nMarlin\nVessel\n10\n\n\nSeawatch\nVessel\n9\n\n\nThe Middleman\nPerson\n8\n\n\nSerenity\nVessel\n8\n\n\nNorthern Light\nVessel\n8\n\n\nHaacklee Harbor\nLocation\n8\n\n\nElise\nPerson\n7\n\n\nThe Lookout\nPerson\n7\n\n\nSailor Shifts Team\nOrganization\n7\n\n\nSamantha Blake\nPerson\n6\n\n\nGlitters Team\nOrganization\n4\n\n\nKnowles\nVessel\n4\n\n\nSmall Fry\nPerson\n3\n\n\nOsprey\nVessel\n3\n\n\nCity Officials\nGroup\n1\n\n\n\n\n\n\n\n\n\n\n\nCentrality Metrics and Direct & Indirect Influences\n\n\n\nBy calculating centrality metrics within Nadia‚Äôs two-hop ego network, we observe that the most influential nodes in her environment‚Äîby PageRank, betweenness, and degree‚Äîare Neptune, V. Miesel Shipping, and Elise. Nadia herself consistently ranks among the top nodes by these measures, highlighting her role as both an influencer and an information bridge. Entities such as Neptune and V. Miesel Shipping, which also score highly in centrality, exert considerable influence over Nadia‚Äôs information flow and access to other parts of the network.\nDegree centrality analysis shows Nadia maintains multiple direct connections, particularly with other highly active nodes, ensuring she is closely linked to key hubs in the network. Betweenness centrality further reveals that Nadia is not only well-connected but also acts as an important intermediary, facilitating communication between otherwise distant parts of the network. PageRank confirms that her immediate environment is composed of actors with significant structural power, increasing the likelihood that Nadia is both influenced by, and exerts influence upon, the most pivotal players in Oceanus.\n\n\n\n\n\n\nCode\nlibrary(igraph)\nlibrary(visNetwork)\n\n# -- Build the global network g as in your earlier code (using your entity_nodes/entity_edges) --\n\ng &lt;- graph_from_data_frame(\n  d = entity_edges, \n  vertices = entity_nodes, \n  directed = TRUE\n)\n\n# -- Get Nadia's index in g --\nnadia_label &lt;- \"Nadia Conti\"\ntarget_index &lt;- which(V(g)$label == nadia_label)\n\n# -- Extract Nadia's 1-hop ego network (all direct neighbors) --\nego_1 &lt;- make_ego_graph(g, order = 1, nodes = target_index, mode = \"all\")[[1]]\n\n\n# 1. Compute PageRank for the ego network\nV(ego_1)$pagerank &lt;- page_rank(ego_1)$vector\n\n# 2. Prepare node data frame with your consistent color scheme\nnodes_df_pagerank &lt;- data.frame(\n  id = V(ego_1)$name,\n  label = V(ego_1)$label,\n  group = V(ego_1)$sub_type,\n  title = paste0(\"&lt;b&gt;\", V(ego_1)$label, \"&lt;/b&gt;&lt;br&gt;PageRank: \", round(V(ego_1)$pagerank, 4)),\n  shape = ifelse(V(ego_1)$sub_type == \"Person\", \"dot\",\n                 ifelse(V(ego_1)$sub_type == \"Organization\", \"square\",\n                        ifelse(V(ego_1)$sub_type == \"Vessel\", \"triangle\",\n                               ifelse(V(ego_1)$sub_type == \"Group\", \"star\", \"diamond\")))),\n  color = case_when(\n    V(ego_1)$sub_type == \"Person\" ~ \"#2ca5ff\",\n    V(ego_1)$sub_type == \"Organization\" ~ \"#f5ee15\",\n    V(ego_1)$sub_type == \"Vessel\" ~ \"#FB7E81\",\n    V(ego_1)$sub_type == \"Group\" ~ \"#25e158\",\n    V(ego_1)$sub_type == \"Location\" ~ \"#ec4bff\",\n    TRUE ~ \"black\"\n  ),\n  value = V(ego_1)$pagerank * 30 + 5\n)\n\n# 3. Prepare edges\nedges_df &lt;- as_data_frame(ego_1, what = \"edges\") %&gt;%\n  rename(from = from, to = to)\n\n# 4. Plot with visNetwork\nvisNetwork(nodes_df_pagerank, edges_df, width = \"100%\", height = \"400px\") %&gt;%\n  visNodes(\n    scaling = list(min = 5, max = 30),\n    color = list(background = nodes_df_pagerank$color, border = \"black\"),\n    shape = nodes_df_pagerank$shape\n  ) %&gt;%\n  visEdges(arrows = list(to = list(enabled = TRUE, scaleFactor = 0.3)), color = list(color = \"gray\")) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE, manipulation = FALSE) %&gt;%\n  visInteraction(dragNodes = FALSE, dragView = FALSE, zoomView = FALSE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 1818)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 1. Compute Betweenness for the ego network\nV(ego_1)$betweenness &lt;- betweenness(ego_1, directed = TRUE)\n\n# 2. Prepare node data frame\nnodes_df_betweenness &lt;- data.frame(\n  id = V(ego_1)$name,\n  label = V(ego_1)$label,\n  group = V(ego_1)$sub_type,\n  title = paste0(\"&lt;b&gt;\", V(ego_1)$label, \"&lt;/b&gt;&lt;br&gt;Betweenness: \", round(V(ego_1)$betweenness, 2)),\n  shape = ifelse(V(ego_1)$sub_type == \"Person\", \"dot\",\n                 ifelse(V(ego_1)$sub_type == \"Organization\", \"square\",\n                        ifelse(V(ego_1)$sub_type == \"Vessel\", \"triangle\",\n                               ifelse(V(ego_1)$sub_type == \"Group\", \"star\", \"diamond\")))),\n  color = case_when(\n    V(ego_1)$sub_type == \"Person\" ~ \"#2ca5ff\",\n    V(ego_1)$sub_type == \"Organization\" ~ \"#f5ee15\",\n    V(ego_1)$sub_type == \"Vessel\" ~ \"#FB7E81\",\n    V(ego_1)$sub_type == \"Group\" ~ \"#25e158\",\n    V(ego_1)$sub_type == \"Location\" ~ \"#ec4bff\",\n    TRUE ~ \"black\"\n  ),\n  value = V(ego_1)$betweenness * 2 + 5\n)\n\n# 3. Edges (same as before)\n# edges_df already prepared\n\n# 4. Plot\nvisNetwork(nodes_df_betweenness, edges_df, width = \"100%\", height = \"400px\") %&gt;%\n  visNodes(\n    scaling = list(min = 5, max = 30),\n    color = list(background = nodes_df_betweenness$color, border = \"black\"),\n    shape = nodes_df_betweenness$shape\n  ) %&gt;%\n  visEdges(arrows = list(to = list(enabled = TRUE, scaleFactor = 0.3)), color = list(color = \"gray\")) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE, manipulation = FALSE) %&gt;%\n  visInteraction(dragNodes = FALSE, dragView = FALSE, zoomView = FALSE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 1818)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 1. Compute Degree for the ego network\nV(ego_1)$degree &lt;- degree(ego_1, mode = \"all\")\n\n# 2. Prepare node data frame\nnodes_df_degree &lt;- data.frame(\n  id = V(ego_1)$name,\n  label = V(ego_1)$label,\n  group = V(ego_1)$sub_type,\n  title = paste0(\"&lt;b&gt;\", V(ego_1)$label, \"&lt;/b&gt;&lt;br&gt;Degree: \", round(V(ego_1)$degree, 2)),\n  shape = ifelse(V(ego_1)$sub_type == \"Person\", \"dot\",\n                 ifelse(V(ego_1)$sub_type == \"Organization\", \"square\",\n                        ifelse(V(ego_1)$sub_type == \"Vessel\", \"triangle\",\n                               ifelse(V(ego_1)$sub_type == \"Group\", \"star\", \"diamond\")))),\n  color = case_when(\n    V(ego_1)$sub_type == \"Person\" ~ \"#2ca5ff\",\n    V(ego_1)$sub_type == \"Organization\" ~ \"#f5ee15\",\n    V(ego_1)$sub_type == \"Vessel\" ~ \"#FB7E81\",\n    V(ego_1)$sub_type == \"Group\" ~ \"#25e158\",\n    V(ego_1)$sub_type == \"Location\" ~ \"#ec4bff\",\n    TRUE ~ \"black\"\n  ),\n  value = V(ego_1)$degree * 5 + 5\n)\n\n# 3. Edges (same as before)\n# edges_df already prepared\n\n# 4. Plot\nvisNetwork(nodes_df_degree, edges_df, width = \"100%\", height = \"400px\") %&gt;%\n  visNodes(\n    scaling = list(min = 5, max = 30),\n    color = list(background = nodes_df_degree$color, border = \"black\"),\n    shape = nodes_df_degree$shape\n  ) %&gt;%\n  visEdges(arrows = list(to = list(enabled = TRUE, scaleFactor = 0.3)), color = list(color = \"gray\")) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE, manipulation = FALSE) %&gt;%\n  visInteraction(dragNodes = FALSE, dragView = FALSE, zoomView = FALSE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 1818)\n\n\n\n\n\n\n\n\n\n\n\n\n1c. With a focus on ‚ÄúNadia Conti‚Äù, the visuals above could determine who has influence over this person.\n\n\n\n\nDegree centrality reveals that Nadia Conti is well-connected within her local network, with a degree of 17. However, she is not the most connected node; vessels such as Mako (37), Reef Guardian (27), and Remora (21), as well as organizations like Oceanus City Council (28) and V. Miesel Shipping (19), have even higher degrees. This indicates that while Nadia is an important hub, her sphere of direct interaction is embedded within a dense mesh of other highly connected entities.\nSeveral other individuals (e.g., Davis with 16, Boss with 13, Mrs.¬†Money with 12) and vessels (e.g., Neptune with 19, Sentinel with 16) also play significant roles in Nadia‚Äôs network. The presence of organizations (e.g., Green Guardians, Sailor Shifts Team), multiple vessels, and key persons shows that Nadia‚Äôs environment is both diverse and robust.\nDirect Connections\nThese direct connections are clearly shown as nodes that have edges (arrows) going into or out of Nadia Conti‚Äôs node in the network diagrams. Nadia Conti directly connects to several core entities across different types:\n\nPeople: Elise, Liam Thorne, Davis, Rodriguez\nOrganization: V. Miesel Shipping, Oceanus City Council, Sailor Shifts Team\nVessel: Neptune, Marlin, Remora, Sentinel\nLocation: Haacklee Harbor\n\nInterpretation: The PageRank, Betweenness, and Degree centrality plots all consistently show Nadia Conti as a major hub, with a large node size reflecting her high centrality. Her immediate network includes influential vessels (Neptune, Remora), organizations (V. Miesel Shipping, Oceanus City Council), and several persons (Elise, Davis, Rodriguez).\nNadia‚Äôs position suggests she is a key connector and influencer but is herself surrounded by even larger hubs, particularly among vessels and organizations. Her ability to influence‚Äîand be influenced‚Äîis amplified by these connections, as these high-degree entities are likely sources and conduits of critical information and operational coordination. This structure points to a tightly interwoven community, where central actors such as Mako, Oceanus City Council, and V. Miesel Shipping may exert the most substantial influence over Nadia‚Äôs access to information, resources, and strategic decisions."
  }
]